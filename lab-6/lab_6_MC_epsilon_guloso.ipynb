{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8441f52a",
      "metadata": {
        "id": "8441f52a"
      },
      "source": [
        "# Experimento 6: MC epsilon guloso"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4298bf0e",
      "metadata": {
        "id": "4298bf0e"
      },
      "source": [
        "## Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4786f23b",
      "metadata": {
        "id": "4786f23b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sns\n",
        "from typing import Dict, Tuple, List, Union\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8610559",
      "metadata": {
        "id": "e8610559"
      },
      "source": [
        "## Ambiente: Navegação no Labirinto (gridworld)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2766bd79",
      "metadata": {
        "id": "2766bd79"
      },
      "outputs": [],
      "source": [
        "class AmbienteNavegacaoLabirinto:\n",
        "    def __init__(self, world_size, bad_states, target_states, allow_bad_entry=False, rewards=[-1, -1, 1, 0]):\n",
        "        \"\"\"\n",
        "        Inicializa o ambiente de navegação em labirinto.\n",
        "\n",
        "        Parâmetros:\n",
        "        - world_size: tupla (n_linhas, n_colunas)\n",
        "        - bad_states: lista de tuplas com coordenadas de estados penalizados\n",
        "        - target_states: lista de tuplas com coordenadas dos estados de objetivo\n",
        "        - allow_bad_entry: bool, se False impede entrada em estados ruins (rebote)\n",
        "        - rewards: lista de recompensas com [r_boundary, r_bad, r_target, r_other]\n",
        "        \"\"\"\n",
        "        self.n_rows, self.n_cols = world_size   # dimensões da grade do labirinto\n",
        "        self.bad_states = set(bad_states)       # estados com penalidade alta\n",
        "        self.target_states = set(target_states) # estados com recompensa alta\n",
        "        self.allow_bad_entry = allow_bad_entry  # se o agente pode entrar em estados ruins\n",
        "\n",
        "        # Recompensas definidas para cada tipo de transição\n",
        "        self.r_boundary = rewards[0]  # tentar sair da grade\n",
        "        self.r_bad = rewards[1]       # transição para estado ruim\n",
        "        self.r_target = rewards[2]    # transição para estado alvo\n",
        "        self.r_other = rewards[3]     # demais transições\n",
        "\n",
        "        # Espaço de ações: dicionário com deslocamentos (linha, coluna)\n",
        "        self.action_space = {\n",
        "            0: (-1, 0),  # cima\n",
        "            1: (1, 0),   # baixo\n",
        "            2: (0, -1),  # esquerda\n",
        "            3: (0, 1),   # direita\n",
        "            4: (0, 0)    # permanecer no mesmo estado\n",
        "        }\n",
        "\n",
        "        # Espaço de recompensas: lista de recompensas possíveis\n",
        "        self.recompensas_possiveis = np.array(sorted(set(rewards)))\n",
        "        self.reward_map = {r: i for i, r in enumerate(self.recompensas_possiveis)}\n",
        "\n",
        "        # número total de estados\n",
        "        self.n_states = self.n_rows * self.n_cols\n",
        "\n",
        "        # número total de ações\n",
        "        self.n_actions = len(self.action_space)\n",
        "\n",
        "        # número total de recompensas possíveis\n",
        "        self.n_rewards = self.recompensas_possiveis.shape[0]\n",
        "\n",
        "        # Tensor de probabilidades de transição: P(s'|s,a)\n",
        "        self.state_transition_probabilities = np.zeros((self.n_states, self.n_states, self.n_actions))\n",
        "\n",
        "        # Tensor de probabilidade de recompensas: P(r|s,a)\n",
        "        self.reward_probabilities = np.zeros((self.n_rewards, self.n_states, self.n_actions))\n",
        "\n",
        "        # Matriz de recompensa imediata (determinística): recompensa[s, a] = r\n",
        "        self.recompensas_imediatas = np.zeros((self.n_states, self.n_actions))\n",
        "\n",
        "        # Matriz de transição de estados (determinística): transicao[s, a] = s'\n",
        "        self.transicao_de_estados = np.zeros((self.n_states, self.n_actions), dtype=int)\n",
        "\n",
        "        self.agent_pos = (0, 0)  # posição inicial do agente\n",
        "\n",
        "        self._init_dynamics()  # inicializa as dinâmicas de transição e recompensa\n",
        "\n",
        "\n",
        "    def _init_dynamics(self):\n",
        "        \"\"\"\n",
        "        Preenche as matrizes de transição e recompensa com base\n",
        "        na estrutura do ambiente e regras de movimentação.\n",
        "        \"\"\"\n",
        "        for indice_estado in range(self.n_states):\n",
        "            estado_atual = self.index_to_state(indice_estado)\n",
        "\n",
        "            for acao, (d_linha, d_coluna) in self.action_space.items():\n",
        "                proxima_posicao = (estado_atual[0] + d_linha, estado_atual[1] + d_coluna)\n",
        "\n",
        "                # Verifica se o movimento é válido ou resulta em rebote\n",
        "                if not self._in_bounds(proxima_posicao) or (not self.allow_bad_entry and proxima_posicao in self.bad_states):\n",
        "                    proximo_estado = estado_atual  # rebote: permanece no estado atual\n",
        "                else:\n",
        "                    proximo_estado = proxima_posicao\n",
        "\n",
        "                # Calcula a recompensa imediata da transição (s, a)\n",
        "                recompensa = self._compute_reward(proxima_posicao)\n",
        "\n",
        "                # Armazena a recompensa imediata na matriz\n",
        "                self.recompensas_imediatas[indice_estado, acao] = recompensa\n",
        "\n",
        "                # Ambiente determinístico\n",
        "                indice_proximo = self.state_to_index(proximo_estado)\n",
        "                self.state_transition_probabilities[indice_proximo, indice_estado, acao] = 1.0  # registra probabilidade P(s'|s,a)\n",
        "                indice_recompensa = self.reward_map[recompensa]\n",
        "                self.reward_probabilities[indice_recompensa, indice_estado, acao] = 1.0  # registra probabilidade P(r|s,a)\n",
        "\n",
        "                # Armazena transição determinística (s, a) -> s'\n",
        "                self.transicao_de_estados[indice_estado, acao] = indice_proximo\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia a posição do agente para o estado inicial (0, 0).\"\"\"\n",
        "        self.agent_pos = (0, 0)\n",
        "        return self.agent_pos\n",
        "\n",
        "\n",
        "    def step(self, acao: int, *, linear: bool = False) -> Tuple[Union[int, Tuple[int, int]], float]:\n",
        "        \"\"\"\n",
        "        Executa uma ação no ambiente, atualiza a posição do agente e devolve o próximo estado no formato desejado.\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        acao   : int\n",
        "            Índice da ação a ser executada (0‒4).\n",
        "        linear : bool, opcional (default = False)\n",
        "            False -> retorna o estado como tupla (linha, coluna).\n",
        "            True  -> retorna o estado como índice linear {0,...,self.n_states-1}.\n",
        "\n",
        "        Retorna\n",
        "        -------\n",
        "        proximo_estado : (linha, coluna) | int\n",
        "            Nova posição do agente (próximo estado) no formato especificado.\n",
        "        recompensa     : float\n",
        "            Recompensa imediata recebida.\n",
        "        \"\"\"\n",
        "        d_linha, d_coluna = self.action_space[acao]\n",
        "        linha_dest = self.agent_pos[0] + d_linha\n",
        "        coluna_dest = self.agent_pos[1] + d_coluna\n",
        "        destino = (linha_dest, coluna_dest)\n",
        "\n",
        "        # Rebote se fora dos limites ou entrada em bad_state proibida\n",
        "        if not self._in_bounds(destino) or (not self.allow_bad_entry and destino in self.bad_states):\n",
        "            destino = self.agent_pos  # permanece\n",
        "\n",
        "        recompensa = self._compute_reward(destino)\n",
        "        self.agent_pos = destino\n",
        "\n",
        "        # --- Formato de retorno ---\n",
        "        proximo_estado = self.state_to_index(destino) if linear else destino\n",
        "\n",
        "        return proximo_estado, recompensa\n",
        "\n",
        "\n",
        "    def _in_bounds(self, posicao):\n",
        "        \"\"\"Verifica se uma posição está dentro dos limites do labirinto.\"\"\"\n",
        "        linha, coluna = posicao\n",
        "        return 0 <= linha < self.n_rows and 0 <= coluna < self.n_cols\n",
        "\n",
        "\n",
        "    def _compute_reward(self, destino):\n",
        "        \"\"\"\n",
        "        Define a recompensa com base no destino proposto:\n",
        "        - r_boundary: fora do grid\n",
        "        - r_bad: célula ruim\n",
        "        - r_target: célula alvo\n",
        "        - r_other: demais casos\n",
        "        \"\"\"\n",
        "        if not self._in_bounds(destino):\n",
        "            return self.r_boundary\n",
        "        elif destino in self.bad_states:\n",
        "            return self.r_bad\n",
        "        elif destino in self.target_states:\n",
        "            return self.r_target\n",
        "        else:\n",
        "            return self.r_other\n",
        "\n",
        "\n",
        "    def reset_to_state(self, state, verify_bounds: bool = True):\n",
        "        \"\"\"\n",
        "        Teleporta o agente para 'state' sem reiniciar o episódio completo.\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        state : (int, int) | int\n",
        "            - Tupla (linha, coluna)\n",
        "            ou\n",
        "            - Índice linear (int)\n",
        "        verify_bounds : bool\n",
        "            Se True, lança ValueError se o estado não for válido.\n",
        "\n",
        "        Retorna\n",
        "        -------\n",
        "        observation : object\n",
        "            A observação correspondente ao novo estado (a própria posição).\n",
        "        \"\"\"\n",
        "        # Converte índice -> tupla, se necessário\n",
        "        if isinstance(state, int):\n",
        "            state = self.index_to_state(state)\n",
        "\n",
        "        if verify_bounds and not self._in_bounds(state):\n",
        "            raise ValueError(f\"Estado {state} fora dos limites do labirinto\")\n",
        "\n",
        "        self.agent_pos = tuple(state)      # mantém tupla imutável\n",
        "\n",
        "        return self.agent_pos\n",
        "\n",
        "\n",
        "    def state_to_index(self, estado):\n",
        "        \"\"\"Converte coordenada (linha, coluna) para índice linear.\"\"\"\n",
        "        linha, coluna = estado\n",
        "        return linha * self.n_cols + coluna\n",
        "\n",
        "\n",
        "    def index_to_state(self, indice):\n",
        "        \"\"\"Converte índice linear para coordenada (linha, coluna).\"\"\"\n",
        "        return divmod(indice, self.n_cols)  # (linha, coluna) = (indice // self.n_cols, indice % self.n_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e100bbcf",
      "metadata": {
        "id": "e100bbcf"
      },
      "source": [
        "## Funções auxiliares para visualização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eedf6090",
      "metadata": {
        "id": "eedf6090"
      },
      "outputs": [],
      "source": [
        "def plot_policy(env, policy, ax=None, titulo=\"Política\"):\n",
        "    _, ax = _prepare_grid(env, ax=ax)\n",
        "\n",
        "    for (r, c), action in policy.items():\n",
        "        x, y = c + 0.5, r + 0.5\n",
        "        color = 'black'\n",
        "        lw = 1.5\n",
        "\n",
        "        if action == 0:\n",
        "            ax.arrow(x, y, dx=0, dy=-0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
        "        elif action == 1:\n",
        "            ax.arrow(x, y, dx=0, dy=0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
        "        elif action == 2:\n",
        "            ax.arrow(x, y, dx=-0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
        "        elif action == 3:\n",
        "            ax.arrow(x, y, dx=0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
        "        elif action == 4:\n",
        "            circ = patches.Circle((x, y), 0.1, edgecolor=color, facecolor='none', linewidth=lw)\n",
        "            ax.add_patch(circ)\n",
        "\n",
        "    ax.set_title(titulo)\n",
        "    if ax is None:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def _prepare_grid(env, ax=None, draw_cells=True):\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(env.n_cols, env.n_rows))\n",
        "    ax.set_xlim(0, env.n_cols)\n",
        "    ax.set_ylim(0, env.n_rows)\n",
        "    ax.set_xticks(np.arange(0, env.n_cols + 1, 1))\n",
        "    ax.set_yticks(np.arange(0, env.n_rows + 1, 1))\n",
        "    ax.grid(True)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    if draw_cells:\n",
        "        for r in range(env.n_rows):\n",
        "            for c in range(env.n_cols):\n",
        "                cell = (r, c)\n",
        "                if cell in env.bad_states:\n",
        "                    color = 'red'\n",
        "                elif cell in env.target_states:\n",
        "                    color = 'green'\n",
        "                else:\n",
        "                    color = 'white'\n",
        "                rect = patches.Rectangle(xy=(c, r), width=1, height=1, facecolor=color, edgecolor='gray')\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "    return (None, ax) if ax else (fig, ax)\n",
        "\n",
        "def plot_valores_de_estado(valores_estado, ambiente, ax=None, titulo=\"Valores de Estado (V(s))\", cbar=True):\n",
        "\n",
        "    dados = valores_estado.reshape(ambiente.n_rows, ambiente.n_cols)\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(ambiente.n_cols, ambiente.n_rows))\n",
        "\n",
        "    sns.heatmap(\n",
        "        data=dados,\n",
        "        annot=True,\n",
        "        fmt='.1f',\n",
        "        cmap='bwr',\n",
        "        square=True,\n",
        "        cbar=cbar,\n",
        "        linewidths=0.5,\n",
        "        linecolor='gray',\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(titulo)\n",
        "    if ax is None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_valores_de_acao(valores_de_acao):\n",
        "    Q_transposta = valores_de_acao.T\n",
        "    n_acoes, n_estados = Q_transposta.shape\n",
        "\n",
        "    plt.figure(figsize=(n_estados, n_acoes))\n",
        "    ax = sns.heatmap(\n",
        "        Q_transposta,\n",
        "        annot=True,\n",
        "        fmt='.1f',\n",
        "        cmap='bwr',\n",
        "        cbar=True,\n",
        "        square=False,\n",
        "        linewidths=0.5,\n",
        "        linecolor='gray'\n",
        "    )\n",
        "    # Rótulos das colunas (estados)\n",
        "    ax.set_xticks(np.arange(n_estados) + 0.5)\n",
        "    ax.set_xticklabels([f\"s{i}\" for i in range(n_estados)], rotation=0)\n",
        "\n",
        "    # Rótulos das linhas (ações)\n",
        "    ax.set_yticks(np.arange(n_acoes) + 0.5)\n",
        "    ax.set_yticklabels([f\"a{i}\" for i in range(n_acoes)], rotation=0)\n",
        "\n",
        "    ax.set_xlabel(r\"Estados\")\n",
        "    ax.set_ylabel(r\"Ações\")\n",
        "    ax.set_title(r\"Valores de ação (Q(s, a) transposta)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_labirinto(ambiente):\n",
        "    \"\"\"\n",
        "    Visualiza o labirinto usando seaborn.heatmap sem ticks nos eixos.\n",
        "\n",
        "    Representa:\n",
        "    - Estado neutro: branco\n",
        "    - Estado ruim: vermelho\n",
        "    - Estado alvo: verde\n",
        "    \"\"\"\n",
        "    # Cria matriz com valores padrão (0 = neutro)\n",
        "    matriz = np.zeros((ambiente.n_rows, ambiente.n_cols), dtype=int)\n",
        "\n",
        "    # Marca os estados ruins como 1\n",
        "    for (r, c) in ambiente.bad_states:\n",
        "        matriz[r, c] = 1\n",
        "\n",
        "    # Marca os estados alvo como 2\n",
        "    for (r, c) in ambiente.target_states:\n",
        "        matriz[r, c] = 2\n",
        "\n",
        "    # Mapa de cores: branco = neutro, vermelho = ruim, verde = alvo\n",
        "    cmap = ListedColormap([\"white\", \"red\", \"green\"])\n",
        "\n",
        "    plt.figure(figsize=(ambiente.n_cols, ambiente.n_rows))\n",
        "    ax = sns.heatmap(\n",
        "        matriz,\n",
        "        cmap=cmap,\n",
        "        cbar=False,\n",
        "        linewidths=0.5,\n",
        "        linecolor='gray',\n",
        "        square=True\n",
        "    )\n",
        "\n",
        "    # Remove todos os ticks e labels\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "    ax.set_title(\"Visualização do Labirinto\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_visitas_log(n_visitas):\n",
        "    \"\"\"\n",
        "    Gera um gráfico de dispersão com escala logarítmica no eixo y\n",
        "    mostrando o número de visitas para cada par (s,a).\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    n_visitas : np.ndarray\n",
        "        Matriz de número de visitas de shape (n_states, n_actions).\n",
        "    \"\"\"\n",
        "    n_states, n_actions = n_visitas.shape\n",
        "    x = np.arange(n_states * n_actions)  # índice linear do par (s,a)\n",
        "    y = n_visitas.flatten()              # número de visitas\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.scatter(x, y, s=10, alpha=0.7)\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel(\"Índice linear do par (s,a)\")\n",
        "    plt.ylabel(\"Número de visitas ao par (s,a)\")\n",
        "    plt.title(\"Frequência de visitas (escala log)\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tEVQ_XsYTMCi",
      "metadata": {
        "id": "tEVQ_XsYTMCi"
      },
      "source": [
        "## MC epsilon-gulosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UmGY2GCRiom0",
      "metadata": {
        "id": "UmGY2GCRiom0"
      },
      "outputs": [],
      "source": [
        "def gerar_episodio_eps(\n",
        "    ambiente,\n",
        "    politica: Dict[int, np.ndarray],\n",
        "    T: int,\n",
        "    *,\n",
        "    disable_tqdm: bool = False\n",
        ") -> List[Tuple[int, int, float]]:\n",
        "    \"\"\"\n",
        "    Gera episódio de comprimento fixo T seguindo política epsilon-suave.\n",
        "\n",
        "    ----------\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    ambiente : AmbienteNavegacaoLabirinto\n",
        "    politica : Dict[int, np.ndarray]\n",
        "        Política estocástica atual: para cada estado, um vetor de probabilidades sobre as ações.\n",
        "    T : int\n",
        "        Horizonte fixo.\n",
        "    disable_tqdm: bool\n",
        "        Desabilitar a barra de progresso.\n",
        "\n",
        "    ----------\n",
        "    Retorno\n",
        "    -------\n",
        "    episodio\n",
        "    \"\"\"\n",
        "\n",
        "    # Codigo Aqui\n",
        "\n",
        "    return episodio\n",
        "\n",
        "\n",
        "def mc_epsilon_guloso(\n",
        "    ambiente,\n",
        "    gamma: float = 0.9,\n",
        "    n_episodios: int = 10_000,\n",
        "    horizonte_T: int = 50,\n",
        "    eps: float = 0.1\n",
        ") -> Tuple[np.ndarray,\n",
        "           Dict[int,np.ndarray],\n",
        "           np.ndarray,\n",
        "           List[float]]:\n",
        "    \"\"\"\n",
        "    Implementa o MC epsilon-guloso para o AmbienteNavegacaoLabirinto.\n",
        "\n",
        "    Para cada episódio:\n",
        "      1. Gera um episódio de comprimento fixo (horizonte_T) seguindo uma política epsilon-suave;\n",
        "      2. Percorre a trajetória de trás para frente, acumulando o retorno G e atualizando a estimativa de Q(s,a);\n",
        "      3. Após cada atualização de Q(s_t, . ), executa melhoria de política (determinística e gulosa).\n",
        "\n",
        "    A política determinística é convertida para {(linha, coluna): ação} apenas no retorno.\n",
        "\n",
        "    ----------\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    ambiente    : AmbienteNavegacaoLabirinto\n",
        "        Gridworld determinístico onde o agente navega.\n",
        "    gamma       : float\n",
        "        Fator de desconto.\n",
        "    n_episodios : int\n",
        "        Número total de episódios a serem executados.\n",
        "    horizonte_T : int\n",
        "        Comprimento fixo de cada episódio.\n",
        "    eps         : float\n",
        "        Parâmetro de suavização da política ε-suave.\n",
        "\n",
        "    ----------\n",
        "    Retorno\n",
        "    ----------\n",
        "    Q                 : np.ndarray, shape = (n_states, n_actions)\n",
        "        Estimativas finais de Q(s,a) obtidas por média de retornos.\n",
        "    politica          : Dict[(int,int), int]\n",
        "        Política determinística no formato {(linha, coluna): ação}.\n",
        "    numero_de_visitas : np.ndarray, shape = (n_states, n_actions)\n",
        "        Matriz com o número de visitas a cada par (s,a) durante todo o processo.\n",
        "    \"\"\"\n",
        "\n",
        "    n_estados, n_acoes = ambiente.n_states, ambiente.n_actions\n",
        "\n",
        "    # ---------- Inicializações ----------\n",
        "    Q = np.zeros((n_estados, n_acoes))\n",
        "    soma_dos_retornos = np.zeros((n_estados, n_acoes))\n",
        "    numero_de_visitas = np.zeros((n_estados, n_acoes))\n",
        "\n",
        "    politica: Dict[int, np.ndarray] = {s: np.full(n_acoes, 1.0 / n_acoes) for s in range(n_estados)}  # distribuição uniforme, todas as ações com probabilidade 0.2\n",
        "\n",
        "    retornos = []\n",
        "\n",
        "    # Codigo Aqui\n",
        "\n",
        "    return Q, politica, numero_de_visitas, retornos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec7ff6b9",
      "metadata": {
        "id": "ec7ff6b9"
      },
      "source": [
        "## Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d196b06c",
      "metadata": {
        "id": "d196b06c"
      },
      "outputs": [],
      "source": [
        "# Instancia o ambiente\n",
        "ambiente = AmbienteNavegacaoLabirinto(\n",
        "        world_size=(5, 5),\n",
        "        bad_states=[(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)],\n",
        "        target_states=[(3, 2)],\n",
        "        allow_bad_entry=True,           # permite entrar em estados ruins\n",
        "        rewards=[-1, -1, 1, 0]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80e3daf",
      "metadata": {
        "id": "b80e3daf"
      },
      "source": [
        "## Experimento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QyTnKXQLWlyd",
      "metadata": {
        "id": "QyTnKXQLWlyd"
      },
      "outputs": [],
      "source": [
        "# Simulação\n",
        "Q_eps, politica_eps, n_visitas_eps, retornos = mc_epsilon_guloso(\n",
        "    ambiente,        # gridworld\n",
        "    gamma=0.9,       # fator de desconto\n",
        "    n_episodios=200,  # número total de episódios\n",
        "    horizonte_T=500,  # comprimento fixo de cada episódio\n",
        "    eps=.2          # parâmetro epsilon da política epsilon-gulosa\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pfg9_po8ivZ7",
      "metadata": {
        "id": "Pfg9_po8ivZ7"
      },
      "outputs": [],
      "source": [
        "# Visualização das ações com maior probabilidade e dos valores de ação estimados para a política estocástica\n",
        "politica_deterministica = {ambiente.index_to_state(s): np.argmax(probabilidades) for s, probabilidades in politica_eps.items()}\n",
        "plot_policy(ambiente, politica_deterministica, titulo=r\"Ações com maior probabilidade - MC $\\epsilon$-guloso\")\n",
        "plot_valores_de_acao(Q_eps)\n",
        "plot_visitas_log(n_visitas)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefa\n",
        "1. Implementar o método MC epsilon-guloso.\n",
        "\n",
        "2. Analise o impacto o comprimento do episódio (T). Fixe n_episodios, o fator de desconto, o parâmetro epsilon e varie T.\n",
        "\n",
        "3. Analise o impacto do número de episódios: Fixe T, o fator de desconto, o parâmetro epsilon e varie n_episodios.\n",
        "\n",
        "4. Analise o impacto do número do parâmetro epsilon: Fixe T, o fator de desconto, episodios e varie epsilon.\n",
        "\n",
        "Após cada experimento, plote:\n",
        "\n",
        "    - número de visitas por (s,a)\n",
        "    \n",
        "    - política aprendida\n",
        "\n",
        "A tarefa deve ser entregue com:\n",
        "- código bem comentado (.ipynb),\n",
        "- gráficos,\n",
        "- breve relatório discutindo as observações."
      ],
      "metadata": {
        "id": "jsWu-B9b3ygP"
      },
      "id": "jsWu-B9b3ygP"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4298bf0e",
        "e8610559",
        "e100bbcf"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GAM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}