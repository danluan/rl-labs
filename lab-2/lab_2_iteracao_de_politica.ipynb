{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8441f52a",
   "metadata": {
    "id": "8441f52a"
   },
   "source": [
    "# Experimento 2: Iteração de política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4786f23b",
   "metadata": {
    "executionInfo": {
     "elapsed": 3777,
     "status": "ok",
     "timestamp": 1744668288331,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "4786f23b"
   },
   "outputs": [],
   "source": [
    "# Importações\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2766bd79",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1744668288336,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "2766bd79"
   },
   "outputs": [],
   "source": [
    "# Ambiente: Navegação no Labirinto (gridworld)\n",
    "\n",
    "class AmbienteNavegacaoLabirinto:\n",
    "    def __init__(self, world_size, bad_states, target_states, allow_bad_entry=False, rewards=[-1, -1, 1, 0]):\n",
    "        \"\"\"\n",
    "        Inicializa o ambiente de navegação em labirinto.\n",
    "\n",
    "        Parâmetros:\n",
    "        - world_size: tupla (n_linhas, n_colunas)\n",
    "        - bad_states: lista de tuplas com coordenadas de estados penalizados\n",
    "        - target_states: lista de tuplas com coordenadas dos estados de objetivo\n",
    "        - allow_bad_entry: bool, se False impede entrada em estados ruins (rebote)\n",
    "        - rewards: lista de recompensas com [r_boundary, r_bad, r_target, r_other]\n",
    "        \"\"\"\n",
    "        self.n_rows, self.n_cols = world_size   # dimensões da grade do labirinto\n",
    "        self.bad_states = set(bad_states)       # estados com penalidade alta\n",
    "        self.target_states = set(target_states) # estados com recompensa alta\n",
    "        self.allow_bad_entry = allow_bad_entry  # se o agente pode entrar em estados ruins\n",
    "\n",
    "        # Recompensas definidas para cada tipo de transição\n",
    "        self.r_boundary = rewards[0]  # tentar sair da grade\n",
    "        self.r_bad = rewards[1]       # transição para estado ruim\n",
    "        self.r_target = rewards[2]    # transição para estado alvo\n",
    "        self.r_other = rewards[3]     # demais transições\n",
    "\n",
    "        # Espaço de ações: dicionário com deslocamentos (linha, coluna)\n",
    "        self.action_space = {\n",
    "            0: (-1, 0),  # cima\n",
    "            1: (1, 0),   # baixo\n",
    "            2: (0, -1),  # esquerda\n",
    "            3: (0, 1),   # direita\n",
    "            4: (0, 0)    # permanecer no mesmo estado\n",
    "        }\n",
    "\n",
    "        # Espaço de recompensas: lista de recompensas possíveis\n",
    "        self.recompensas_possiveis = np.array(sorted(set(rewards)))\n",
    "        self.reward_map = {r: i for i, r in enumerate(self.recompensas_possiveis)}\n",
    "\n",
    "        # número total de estados\n",
    "        self.n_states = self.n_rows * self.n_cols\n",
    "\n",
    "        # número total de ações\n",
    "        self.n_actions = len(self.action_space)\n",
    "\n",
    "        # número total de recompensas possíveis\n",
    "        self.n_rewards = self.recompensas_possiveis.shape[0]\n",
    "\n",
    "        # Tensor de probabilidades de transição: P(s'|s,a)\n",
    "        self.state_transition_probabilities = np.zeros((self.n_states, self.n_states, self.n_actions))\n",
    "\n",
    "        # Tensor de probabilidade de recompensas: P(r|s,a)\n",
    "        self.reward_probabilities = np.zeros((self.n_rewards, self.n_states, self.n_actions))\n",
    "\n",
    "        # Matriz de recompensa imediata\n",
    "        self.recompensas_imediatas = np.zeros((self.n_states, self.n_actions))\n",
    "\n",
    "        self.agent_pos = (0, 0)  # posição inicial do agente\n",
    "\n",
    "        self._init_dynamics()  # inicializa as dinâmicas de transição e recompensa\n",
    "\n",
    "\n",
    "    def _init_dynamics(self):\n",
    "        \"\"\"\n",
    "        Preenche as matrizes de transição e recompensa com base\n",
    "        na estrutura do ambiente e regras de movimentação.\n",
    "        \"\"\"\n",
    "        for indice_estado in range(self.n_states):\n",
    "            estado_atual = self.index_to_state(indice_estado)\n",
    "\n",
    "            for acao, (d_linha, d_coluna) in self.action_space.items():\n",
    "                proxima_posicao = (estado_atual[0] + d_linha, estado_atual[1] + d_coluna)\n",
    "\n",
    "                # Verifica se o movimento é válido ou resulta em rebote\n",
    "                if not self._in_bounds(proxima_posicao) or (not self.allow_bad_entry and proxima_posicao in self.bad_states):\n",
    "                    proximo_estado = estado_atual  # rebote: permanece no estado atual\n",
    "                else:\n",
    "                    proximo_estado = proxima_posicao\n",
    "\n",
    "                # Calcula a recompensa imediata da transição (s, a)\n",
    "                recompensa = self._compute_reward(proxima_posicao)\n",
    "\n",
    "                # Armazena a recompensa imediata na matriz\n",
    "                self.recompensas_imediatas[indice_estado, acao] = recompensa\n",
    "\n",
    "                # Ambiente determinístico\n",
    "                indice_proximo = self.state_to_index(proximo_estado)\n",
    "                self.state_transition_probabilities[indice_proximo, indice_estado, acao] = 1.0  # registra probabilidade P(s'|s,a)\n",
    "                indice_recompensa = self.reward_map[recompensa]\n",
    "                self.reward_probabilities[indice_recompensa, indice_estado, acao] = 1.0  # registra probabilidade P(r|s,a)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia a posição do agente para o estado inicial (0, 0).\"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.agent_pos\n",
    "\n",
    "\n",
    "    def step(self, acao):\n",
    "        \"\"\"\n",
    "        Executa uma ação no ambiente e atualiza a posição do agente.\n",
    "\n",
    "        Parâmetros:\n",
    "        - acao: índice da ação a ser executada (0 a 4)\n",
    "\n",
    "        Retorna:\n",
    "        - nova posição do agente (linha, coluna)\n",
    "        - recompensa recebida\n",
    "        \"\"\"\n",
    "        d_linha, d_coluna = self.action_space[acao]\n",
    "        linha_destino = self.agent_pos[0] + d_linha\n",
    "        coluna_destino = self.agent_pos[1] + d_coluna\n",
    "        destino = (linha_destino, coluna_destino)\n",
    "\n",
    "        # Se movimento for inválido ou entrada proibida, permanece\n",
    "        if not self._in_bounds(destino) or (not self.allow_bad_entry and destino in self.bad_states):\n",
    "            destino = self.agent_pos\n",
    "\n",
    "        recompensa = self._compute_reward(destino)\n",
    "        self.agent_pos = destino\n",
    "        return self.agent_pos, recompensa\n",
    "\n",
    "\n",
    "    def _in_bounds(self, posicao):\n",
    "        \"\"\"Verifica se uma posição está dentro dos limites do labirinto.\"\"\"\n",
    "        linha, coluna = posicao\n",
    "        return 0 <= linha < self.n_rows and 0 <= coluna < self.n_cols\n",
    "\n",
    "\n",
    "    def _compute_reward(self, destino):\n",
    "        \"\"\"\n",
    "        Define a recompensa com base no destino proposto:\n",
    "        - r_boundary: fora do grid\n",
    "        - r_bad: célula ruim\n",
    "        - r_target: célula alvo\n",
    "        - r_other: demais casos\n",
    "        \"\"\"\n",
    "        if not self._in_bounds(destino):\n",
    "            return self.r_boundary\n",
    "        elif destino in self.bad_states:\n",
    "            return self.r_bad\n",
    "        elif destino in self.target_states:\n",
    "            return self.r_target\n",
    "        else:\n",
    "            return self.r_other\n",
    "\n",
    "\n",
    "    def state_to_index(self, estado):\n",
    "        \"\"\"Converte coordenada (linha, coluna) para índice linear.\"\"\"\n",
    "        linha, coluna = estado\n",
    "        return linha * self.n_cols + coluna\n",
    "\n",
    "\n",
    "    def index_to_state(self, indice):\n",
    "        \"\"\"Converte índice linear para coordenada (linha, coluna).\"\"\"\n",
    "        return divmod(indice, self.n_cols)  # (linha, coluna) = (indice // self.n_cols, indice % self.n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eedf6090",
   "metadata": {
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1744668288395,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "eedf6090"
   },
   "outputs": [],
   "source": [
    "# Funções auxiliares para visualização\n",
    "\n",
    "def plot_policy(env, policy, ax=None):\n",
    "    fig, ax = _prepare_grid(env, ax=ax)\n",
    "\n",
    "    for (r, c), action in policy.items():\n",
    "        x, y = c + 0.5, r + 0.5\n",
    "        color = 'black'\n",
    "        lw = 1.5\n",
    "\n",
    "        if action == 0:\n",
    "            ax.arrow(x, y, dx=0, dy=-0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 1:\n",
    "            ax.arrow(x, y, dx=0, dy=0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 2:\n",
    "            ax.arrow(x, y, dx=-0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 3:\n",
    "            ax.arrow(x, y, dx=0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 4:\n",
    "            circ = patches.Circle((x, y), 0.1, edgecolor=color, facecolor='none', linewidth=lw)\n",
    "            ax.add_patch(circ)\n",
    "\n",
    "    ax.set_title(\"Política\")\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "def _prepare_grid(env, ax=None, draw_cells=True):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(env.n_cols, env.n_rows))\n",
    "    ax.set_xlim(0, env.n_cols)\n",
    "    ax.set_ylim(0, env.n_rows)\n",
    "    ax.set_xticks(np.arange(0, env.n_cols + 1, 1))\n",
    "    ax.set_yticks(np.arange(0, env.n_rows + 1, 1))\n",
    "    ax.grid(True)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    if draw_cells:\n",
    "        for r in range(env.n_rows):\n",
    "            for c in range(env.n_cols):\n",
    "                cell = (r, c)\n",
    "                if cell in env.bad_states:\n",
    "                    color = 'red'\n",
    "                elif cell in env.target_states:\n",
    "                    color = 'green'\n",
    "                else:\n",
    "                    color = 'white'\n",
    "                rect = patches.Rectangle(xy=(c, r), width=1, height=1, facecolor=color, edgecolor='gray')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "    return (None, ax) if ax else (fig, ax)\n",
    "\n",
    "\n",
    "def plot_valores_de_estado(valores_estado, ambiente):\n",
    "    plt.figure(figsize=(ambiente.n_rows, ambiente.n_cols))\n",
    "    ax = sns.heatmap(\n",
    "        data=valores_estado.reshape(ambiente.n_rows, ambiente.n_cols),\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='bwr',\n",
    "        square=True,\n",
    "        cbar=True,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "    )\n",
    "    ax.set_title(r\"Valores de Estado (V(s))\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_valores_de_acao(valores_de_acao):\n",
    "    Q_transposta = valores_de_acao.T\n",
    "    n_acoes, n_estados = Q_transposta.shape\n",
    "\n",
    "    plt.figure(figsize=(n_estados, n_acoes))\n",
    "    ax = sns.heatmap(\n",
    "        Q_transposta,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='bwr',\n",
    "        cbar=True,\n",
    "        square=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "    # Rótulos das colunas (estados)\n",
    "    ax.set_xticks(np.arange(n_estados) + 0.5)\n",
    "    ax.set_xticklabels([f\"s{i}\" for i in range(n_estados)], rotation=0)\n",
    "\n",
    "    # Rótulos das linhas (ações)\n",
    "    ax.set_yticks(np.arange(n_acoes) + 0.5)\n",
    "    ax.set_yticklabels([f\"a{i}\" for i in range(n_acoes)], rotation=0)\n",
    "\n",
    "    ax.set_xlabel(r\"Estados\")\n",
    "    ax.set_ylabel(r\"Ações\")\n",
    "    ax.set_title(r\"Valores de ação (Q(s, a) transposta)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_labirinto(ambiente):\n",
    "    \"\"\"\n",
    "    Visualiza o labirinto usando seaborn.heatmap sem ticks nos eixos.\n",
    "\n",
    "    Representa:\n",
    "    - Estado neutro: branco\n",
    "    - Estado ruim: vermelho\n",
    "    - Estado alvo: verde\n",
    "    \"\"\"\n",
    "    # Cria matriz com valores padrão (0 = neutro)\n",
    "    matriz = np.zeros((ambiente.n_rows, ambiente.n_cols), dtype=int)\n",
    "\n",
    "    # Marca os estados ruins como 1\n",
    "    for (r, c) in ambiente.bad_states:\n",
    "        matriz[r, c] = 1\n",
    "\n",
    "    # Marca os estados alvo como 2\n",
    "    for (r, c) in ambiente.target_states:\n",
    "        matriz[r, c] = 2\n",
    "\n",
    "    # Mapa de cores: branco = neutro, vermelho = ruim, verde = alvo\n",
    "    cmap = ListedColormap([\"white\", \"red\", \"green\"])\n",
    "\n",
    "    plt.figure(figsize=(ambiente.n_cols, ambiente.n_rows))\n",
    "    ax = sns.heatmap(\n",
    "        matriz,\n",
    "        cmap=cmap,\n",
    "        cbar=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "        square=True\n",
    "    )\n",
    "\n",
    "    # Remove todos os ticks e labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    ax.set_title(\"Visualização do Labirinto\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d196b06c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1744668289318,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "d196b06c",
    "outputId": "286a3abb-f15f-47c3-c84e-879268a90d2d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instancia o ambiente\n",
    "ambiente = AmbienteNavegacaoLabirinto(\n",
    "        world_size=(5, 5),\n",
    "        bad_states=[(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)],\n",
    "        target_states=[(3, 2)],\n",
    "        allow_bad_entry=True,\n",
    "        rewards=[-1, -10, 1, 0]\n",
    "    )\n",
    "plot_labirinto(ambiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691487a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 35 (3655968875.py, line 58)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn V, Q, politica\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'for' statement on line 35\n"
     ]
    }
   ],
   "source": [
    "def iteracao_de_politica(ambiente, gamma=0.9, theta=1e-6, max_iteracoes=1000):\n",
    "    \"\"\"\n",
    "    Implementa o algoritmo de Iteração de Política para encontrar a política ótima.\n",
    "\n",
    "    Parâmetros:\n",
    "    - ambiente: instância da classe AmbienteNavegacaoLabirinto\n",
    "    - gamma: fator de desconto (0 < gamma <= 1)\n",
    "    - theta: limiar mínimo de variação para considerar convergência\n",
    "    - max_iteracoes: número máximo de iterações de melhoria de política\n",
    "\n",
    "    Retorna:\n",
    "     - vetor de valores de estado V (numpy array) para todos os estados\n",
    "     - matriz de valores de ação Q (numpy array) para todos os pares (estado, ação)\n",
    "     - política ótima (dicionário de estado para ação)\n",
    "    \"\"\"\n",
    "\n",
    "    # Informações úteis do ambiente:\n",
    "    # número de estados: ambiente.n_states\n",
    "    # número de ações: ambiente.n_actions\n",
    "    # número de recompensas (únicas): ambiente.n_rewards\n",
    "    # Tensor de probabilidade de recompensas: P(r|s,a): ambiente.reward_probabilities shape=(ambiente.n_rewards, ambiente.n_states, ambiente.n_actions)\n",
    "    # recompensas: ambiente.recompensas_possiveis (ambiente.recompensas_possiveis[i] com probabilidade ambiente.reward_probabilities[i,s,a])\n",
    "    # Tensor de probabilidades de transição: P(s'|s,a): ambiente.state_transition_probabilities shape=(ambiente.n_states, ambiente.n_states, ambiente.n_actions)\n",
    "    # Converte coordenada (linha, coluna) para índice linear: ambiente.state_to_index(estado)\n",
    "    # Converte índice linear para coordenada (linha, coluna): ambiente.index_to_state(indice)\n",
    "\n",
    "    n_estados = ambiente.n_states\n",
    "    n_acoes = ambiente.n_actions\n",
    "\n",
    "    # Inicialização da política e dos valores de estado\n",
    "    politica = {ambiente.index_to_state(s): np.random.choice(n_acoes) for s in range(n_estados)}\n",
    "    V = np.zeros(n_estados)\n",
    "    Q = np.zeros((n_estados, n_acoes))\n",
    "\n",
    "    for _ in range(max_iteracoes):\n",
    "\n",
    "        ############################################################################################################\n",
    "        # AVALIAÇÃO DA POLÍTICA ATUAL\n",
    "        ############################################################################################################\n",
    "        # Código aqui\n",
    "\n",
    "        # Deve calcular:\n",
    "        # Valores de estado V (numpy array) para todos os estados (shape = (ambiente.n_states, )) da política atual\n",
    "\n",
    "        ############################################################################################################\n",
    "\n",
    "        ############################################################################################################\n",
    "        # MELHORIA DE POLÍTICA\n",
    "        ############################################################################################################\n",
    "        # Código aqui\n",
    "\n",
    "        # Deve calcular:\n",
    "        # Valores de ação Q (numpy array) para todos os estados (shape = (ambiente.n_states, ambiente.n_actions))\n",
    "        # política melhorada como dicionário {estado - tupla: melhor_acao - int 0 a 4} (dica: usar ambiente.index_to_state(estado))\n",
    "\n",
    "        ############################################################################################################\n",
    "\n",
    "    return V, Q, politica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c506b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "error",
     "timestamp": 1744668290000,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "b04c506b",
    "outputId": "3a33c3c8-3393-4d07-ff65-dc868ab42049"
   },
   "outputs": [],
   "source": [
    "V, Q, politica = iteracao_de_politica(ambiente, gamma=0.9, theta=1e-6, max_iteracoes=1000)\n",
    "\n",
    "plot_valores_de_estado(V, ambiente)\n",
    "\n",
    "plot_valores_de_acao(Q)\n",
    "\n",
    "plot_policy(ambiente, politica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8f6d4",
   "metadata": {
    "id": "f6a8f6d4"
   },
   "source": [
    "# Tarefa:\n",
    "\n",
    "1. Modifique os códigos dos algoritmos de iteração de valor e de iteração de política para também retornar a iteração k em que a condição de convergência foi satisfeita.\n",
    "2. Compare os algoritmos de iteração de valor e de iteração de política quanto ao número de iterações utilizadas até a condição de convergência ser satisfeita.\n",
    "\n",
    "Entregar o PDF do notebook no colab (código + relatório em markdown)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
