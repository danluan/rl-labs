{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8441f52a",
   "metadata": {
    "id": "8441f52a"
   },
   "source": [
    "# Experimento 2: Iteração de política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786f23b",
   "metadata": {
    "executionInfo": {
     "elapsed": 3777,
     "status": "ok",
     "timestamp": 1744668288331,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "4786f23b"
   },
   "outputs": [],
   "source": [
    "# Importações\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766bd79",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1744668288336,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "2766bd79"
   },
   "outputs": [],
   "source": [
    "# Ambiente: Navegação no Labirinto (gridworld)\n",
    "\n",
    "class AmbienteNavegacaoLabirinto:\n",
    "    def __init__(self, world_size, bad_states, target_states, allow_bad_entry=False, rewards=[-1, -1, 1, 0]):\n",
    "        \"\"\"\n",
    "        Inicializa o ambiente de navegação em labirinto.\n",
    "\n",
    "        Parâmetros:\n",
    "        - world_size: tupla (n_linhas, n_colunas)\n",
    "        - bad_states: lista de tuplas com coordenadas de estados penalizados\n",
    "        - target_states: lista de tuplas com coordenadas dos estados de objetivo\n",
    "        - allow_bad_entry: bool, se False impede entrada em estados ruins (rebote)\n",
    "        - rewards: lista de recompensas com [r_boundary, r_bad, r_target, r_other]\n",
    "        \"\"\"\n",
    "        self.n_rows, self.n_cols = world_size   # dimensões da grade do labirinto\n",
    "        self.bad_states = set(bad_states)       # estados com penalidade alta\n",
    "        self.target_states = set(target_states) # estados com recompensa alta\n",
    "        self.allow_bad_entry = allow_bad_entry  # se o agente pode entrar em estados ruins\n",
    "\n",
    "        # Recompensas definidas para cada tipo de transição\n",
    "        self.r_boundary = rewards[0]  # tentar sair da grade\n",
    "        self.r_bad = rewards[1]       # transição para estado ruim\n",
    "        self.r_target = rewards[2]    # transição para estado alvo\n",
    "        self.r_other = rewards[3]     # demais transições\n",
    "\n",
    "        # Espaço de ações: dicionário com deslocamentos (linha, coluna)\n",
    "        self.action_space = {\n",
    "            0: (-1, 0),  # cima\n",
    "            1: (1, 0),   # baixo\n",
    "            2: (0, -1),  # esquerda\n",
    "            3: (0, 1),   # direita\n",
    "            4: (0, 0)    # permanecer no mesmo estado\n",
    "        }\n",
    "\n",
    "        # Espaço de recompensas: lista de recompensas possíveis\n",
    "        self.recompensas_possiveis = np.array(sorted(set(rewards)))\n",
    "        self.reward_map = {r: i for i, r in enumerate(self.recompensas_possiveis)}\n",
    "\n",
    "        # número total de estados\n",
    "        self.n_states = self.n_rows * self.n_cols\n",
    "\n",
    "        # número total de ações\n",
    "        self.n_actions = len(self.action_space)\n",
    "\n",
    "        # número total de recompensas possíveis\n",
    "        self.n_rewards = self.recompensas_possiveis.shape[0]\n",
    "\n",
    "        # Tensor de probabilidades de transição: P(s'|s,a)\n",
    "        self.state_transition_probabilities = np.zeros((self.n_states, self.n_states, self.n_actions))\n",
    "\n",
    "        # Tensor de probabilidade de recompensas: P(r|s,a)\n",
    "        self.reward_probabilities = np.zeros((self.n_rewards, self.n_states, self.n_actions))\n",
    "\n",
    "        # Matriz de recompensa imediata\n",
    "        self.recompensas_imediatas = np.zeros((self.n_states, self.n_actions))\n",
    "\n",
    "        self.agent_pos = (0, 0)  # posição inicial do agente\n",
    "\n",
    "        self._init_dynamics()  # inicializa as dinâmicas de transição e recompensa\n",
    "\n",
    "\n",
    "    def _init_dynamics(self):\n",
    "        \"\"\"\n",
    "        Preenche as matrizes de transição e recompensa com base\n",
    "        na estrutura do ambiente e regras de movimentação.\n",
    "        \"\"\"\n",
    "        for indice_estado in range(self.n_states):\n",
    "            estado_atual = self.index_to_state(indice_estado)\n",
    "\n",
    "            for acao, (d_linha, d_coluna) in self.action_space.items():\n",
    "                proxima_posicao = (estado_atual[0] + d_linha, estado_atual[1] + d_coluna)\n",
    "\n",
    "                # Verifica se o movimento é válido ou resulta em rebote\n",
    "                if not self._in_bounds(proxima_posicao) or (not self.allow_bad_entry and proxima_posicao in self.bad_states):\n",
    "                    proximo_estado = estado_atual  # rebote: permanece no estado atual\n",
    "                else:\n",
    "                    proximo_estado = proxima_posicao\n",
    "\n",
    "                # Calcula a recompensa imediata da transição (s, a)\n",
    "                recompensa = self._compute_reward(proxima_posicao)\n",
    "\n",
    "                # Armazena a recompensa imediata na matriz\n",
    "                self.recompensas_imediatas[indice_estado, acao] = recompensa\n",
    "\n",
    "                # Ambiente determinístico\n",
    "                indice_proximo = self.state_to_index(proximo_estado)\n",
    "                self.state_transition_probabilities[indice_proximo, indice_estado, acao] = 1.0  # registra probabilidade P(s'|s,a)\n",
    "                indice_recompensa = self.reward_map[recompensa]\n",
    "                self.reward_probabilities[indice_recompensa, indice_estado, acao] = 1.0  # registra probabilidade P(r|s,a)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia a posição do agente para o estado inicial (0, 0).\"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.agent_pos\n",
    "\n",
    "\n",
    "    def step(self, acao):\n",
    "        \"\"\"\n",
    "        Executa uma ação no ambiente e atualiza a posição do agente.\n",
    "\n",
    "        Parâmetros:\n",
    "        - acao: índice da ação a ser executada (0 a 4)\n",
    "\n",
    "        Retorna:\n",
    "        - nova posição do agente (linha, coluna)\n",
    "        - recompensa recebida\n",
    "        \"\"\"\n",
    "        d_linha, d_coluna = self.action_space[acao]\n",
    "        linha_destino = self.agent_pos[0] + d_linha\n",
    "        coluna_destino = self.agent_pos[1] + d_coluna\n",
    "        destino = (linha_destino, coluna_destino)\n",
    "\n",
    "        # Se movimento for inválido ou entrada proibida, permanece\n",
    "        if not self._in_bounds(destino) or (not self.allow_bad_entry and destino in self.bad_states):\n",
    "            destino = self.agent_pos\n",
    "\n",
    "        recompensa = self._compute_reward(destino)\n",
    "        self.agent_pos = destino\n",
    "        return self.agent_pos, recompensa\n",
    "\n",
    "\n",
    "    def _in_bounds(self, posicao):\n",
    "        \"\"\"Verifica se uma posição está dentro dos limites do labirinto.\"\"\"\n",
    "        linha, coluna = posicao\n",
    "        return 0 <= linha < self.n_rows and 0 <= coluna < self.n_cols\n",
    "\n",
    "\n",
    "    def _compute_reward(self, destino):\n",
    "        \"\"\"\n",
    "        Define a recompensa com base no destino proposto:\n",
    "        - r_boundary: fora do grid\n",
    "        - r_bad: célula ruim\n",
    "        - r_target: célula alvo\n",
    "        - r_other: demais casos\n",
    "        \"\"\"\n",
    "        if not self._in_bounds(destino):\n",
    "            return self.r_boundary\n",
    "        elif destino in self.bad_states:\n",
    "            return self.r_bad\n",
    "        elif destino in self.target_states:\n",
    "            return self.r_target\n",
    "        else:\n",
    "            return self.r_other\n",
    "\n",
    "\n",
    "    def state_to_index(self, estado):\n",
    "        \"\"\"Converte coordenada (linha, coluna) para índice linear.\"\"\"\n",
    "        linha, coluna = estado\n",
    "        return linha * self.n_cols + coluna\n",
    "\n",
    "\n",
    "    def index_to_state(self, indice):\n",
    "        \"\"\"Converte índice linear para coordenada (linha, coluna).\"\"\"\n",
    "        return divmod(indice, self.n_cols)  # (linha, coluna) = (indice // self.n_cols, indice % self.n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf6090",
   "metadata": {
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1744668288395,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "eedf6090"
   },
   "outputs": [],
   "source": [
    "# Funções auxiliares para visualização\n",
    "\n",
    "def plot_policy(env, policy, ax=None):\n",
    "    fig, ax = _prepare_grid(env, ax=ax)\n",
    "\n",
    "    for (r, c), action in policy.items():\n",
    "        x, y = c + 0.5, r + 0.5\n",
    "        color = 'black'\n",
    "        lw = 1.5\n",
    "\n",
    "        if action == 0:\n",
    "            ax.arrow(x, y, dx=0, dy=-0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 1:\n",
    "            ax.arrow(x, y, dx=0, dy=0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 2:\n",
    "            ax.arrow(x, y, dx=-0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 3:\n",
    "            ax.arrow(x, y, dx=0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 4:\n",
    "            circ = patches.Circle((x, y), 0.1, edgecolor=color, facecolor='none', linewidth=lw)\n",
    "            ax.add_patch(circ)\n",
    "\n",
    "    ax.set_title(\"Política\")\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "def _prepare_grid(env, ax=None, draw_cells=True):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(env.n_cols, env.n_rows))\n",
    "    ax.set_xlim(0, env.n_cols)\n",
    "    ax.set_ylim(0, env.n_rows)\n",
    "    ax.set_xticks(np.arange(0, env.n_cols + 1, 1))\n",
    "    ax.set_yticks(np.arange(0, env.n_rows + 1, 1))\n",
    "    ax.grid(True)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    if draw_cells:\n",
    "        for r in range(env.n_rows):\n",
    "            for c in range(env.n_cols):\n",
    "                cell = (r, c)\n",
    "                if cell in env.bad_states:\n",
    "                    color = 'red'\n",
    "                elif cell in env.target_states:\n",
    "                    color = 'green'\n",
    "                else:\n",
    "                    color = 'white'\n",
    "                rect = patches.Rectangle(xy=(c, r), width=1, height=1, facecolor=color, edgecolor='gray')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "    return (None, ax) if ax else (fig, ax)\n",
    "\n",
    "\n",
    "def plot_valores_de_estado(valores_estado, ambiente):\n",
    "    plt.figure(figsize=(ambiente.n_rows, ambiente.n_cols))\n",
    "    ax = sns.heatmap(\n",
    "        data=valores_estado.reshape(ambiente.n_rows, ambiente.n_cols),\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='bwr',\n",
    "        square=True,\n",
    "        cbar=True,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "    )\n",
    "    ax.set_title(r\"Valores de Estado (V(s))\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_valores_de_acao(valores_de_acao):\n",
    "    Q_transposta = valores_de_acao.T\n",
    "    n_acoes, n_estados = Q_transposta.shape\n",
    "\n",
    "    plt.figure(figsize=(n_estados, n_acoes))\n",
    "    ax = sns.heatmap(\n",
    "        Q_transposta,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='bwr',\n",
    "        cbar=True,\n",
    "        square=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "    # Rótulos das colunas (estados)\n",
    "    ax.set_xticks(np.arange(n_estados) + 0.5)\n",
    "    ax.set_xticklabels([f\"s{i}\" for i in range(n_estados)], rotation=0)\n",
    "\n",
    "    # Rótulos das linhas (ações)\n",
    "    ax.set_yticks(np.arange(n_acoes) + 0.5)\n",
    "    ax.set_yticklabels([f\"a{i}\" for i in range(n_acoes)], rotation=0)\n",
    "\n",
    "    ax.set_xlabel(r\"Estados\")\n",
    "    ax.set_ylabel(r\"Ações\")\n",
    "    ax.set_title(r\"Valores de ação (Q(s, a) transposta)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_labirinto(ambiente):\n",
    "    \"\"\"\n",
    "    Visualiza o labirinto usando seaborn.heatmap sem ticks nos eixos.\n",
    "\n",
    "    Representa:\n",
    "    - Estado neutro: branco\n",
    "    - Estado ruim: vermelho\n",
    "    - Estado alvo: verde\n",
    "    \"\"\"\n",
    "    # Cria matriz com valores padrão (0 = neutro)\n",
    "    matriz = np.zeros((ambiente.n_rows, ambiente.n_cols), dtype=int)\n",
    "\n",
    "    # Marca os estados ruins como 1\n",
    "    for (r, c) in ambiente.bad_states:\n",
    "        matriz[r, c] = 1\n",
    "\n",
    "    # Marca os estados alvo como 2\n",
    "    for (r, c) in ambiente.target_states:\n",
    "        matriz[r, c] = 2\n",
    "\n",
    "    # Mapa de cores: branco = neutro, vermelho = ruim, verde = alvo\n",
    "    cmap = ListedColormap([\"white\", \"red\", \"green\"])\n",
    "\n",
    "    plt.figure(figsize=(ambiente.n_cols, ambiente.n_rows))\n",
    "    ax = sns.heatmap(\n",
    "        matriz,\n",
    "        cmap=cmap,\n",
    "        cbar=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "        square=True\n",
    "    )\n",
    "\n",
    "    # Remove todos os ticks e labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    ax.set_title(\"Visualização do Labirinto\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196b06c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1744668289318,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "d196b06c",
    "outputId": "286a3abb-f15f-47c3-c84e-879268a90d2d"
   },
   "outputs": [],
   "source": [
    "# Instancia o ambiente\n",
    "ambiente = AmbienteNavegacaoLabirinto(\n",
    "        world_size=(5, 5),\n",
    "        bad_states=[(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)],\n",
    "        target_states=[(3, 2)],\n",
    "        allow_bad_entry=True,\n",
    "        rewards=[-1, -10, 1, 0]\n",
    "    )\n",
    "plot_labirinto(ambiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691487a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteracao_de_politica(ambiente, gamma=0.9, theta=1e-6, max_iteracoes=1000):\n",
    "    \"\"\"\n",
    "    Implementa o algoritmo de Iteração de Política para encontrar a política ótima.\n",
    "\n",
    "    Parâmetros:\n",
    "    - ambiente: instância da classe AmbienteNavegacaoLabirinto\n",
    "    - gamma: fator de desconto (0 < gamma <= 1)\n",
    "    - theta: limiar mínimo de variação para considerar convergência\n",
    "    - max_iteracoes: número máximo de iterações de melhoria de política\n",
    "\n",
    "    Retorna:\n",
    "     - vetor de valores de estado V (numpy array) para todos os estados\n",
    "     - matriz de valores de ação Q (numpy array) para todos os pares (estado, ação)\n",
    "     - política ótima (dicionário de estado para ação)\n",
    "    \"\"\"\n",
    "\n",
    "    # Informações úteis do ambiente:\n",
    "    # número de estados: ambiente.n_states\n",
    "    # número de ações: ambiente.n_actions\n",
    "    # número de recompensas (únicas): ambiente.n_rewards\n",
    "    # Tensor de probabilidade de recompensas: P(r|s,a): ambiente.reward_probabilities shape=(ambiente.n_rewards, ambiente.n_states, ambiente.n_actions)\n",
    "    # recompensas: ambiente.recompensas_possiveis (ambiente.recompensas_possiveis[i] com probabilidade ambiente.reward_probabilities[i,s,a])\n",
    "    # Tensor de probabilidades de transição: P(s'|s,a): ambiente.state_transition_probabilities shape=(ambiente.n_states, ambiente.n_states, ambiente.n_actions)\n",
    "    # Converte coordenada (linha, coluna) para índice linear: ambiente.state_to_index(estado)\n",
    "    # Converte índice linear para coordenada (linha, coluna): ambiente.index_to_state(indice)\n",
    "\n",
    "    n_estados = ambiente.n_states\n",
    "    n_acoes = ambiente.n_actions\n",
    "\n",
    "    # Inicialização da política e dos valores de estado\n",
    "    politica = {ambiente.index_to_state(s): np.random.choice(n_acoes) for s in range(n_estados)}\n",
    "    V = np.zeros(n_estados)\n",
    "    Q = np.zeros((n_estados, n_acoes))\n",
    "\n",
    "    for _ in range(max_iteracoes):\n",
    "\n",
    "        ############################################################################################################\n",
    "        # AVALIAÇÃO DA POLÍTICA ATUAL\n",
    "        ############################################################################################################\n",
    "        # Código aqui\n",
    "\n",
    "        # Deve calcular:\n",
    "        # Valores de estado V (numpy array) para todos os estados (shape = (ambiente.n_states, )) da política atual\n",
    "\n",
    "        ############################################################################################################\n",
    "\n",
    "        ############################################################################################################\n",
    "        # MELHORIA DE POLÍTICA\n",
    "        ############################################################################################################\n",
    "        # Código aqui\n",
    "\n",
    "        # Deve calcular:\n",
    "        # Valores de ação Q (numpy array) para todos os estados (shape = (ambiente.n_states, ambiente.n_actions))\n",
    "        # política melhorada como dicionário {estado - tupla: melhor_acao - int 0 a 4} (dica: usar ambiente.index_to_state(estado))\n",
    "\n",
    "        ############################################################################################################\n",
    "\n",
    "    return V, Q, politica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c506b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "error",
     "timestamp": 1744668290000,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "b04c506b",
    "outputId": "3a33c3c8-3393-4d07-ff65-dc868ab42049"
   },
   "outputs": [],
   "source": [
    "V, Q, politica = iteracao_de_politica(ambiente, gamma=0.9, theta=1e-6, max_iteracoes=1000)\n",
    "\n",
    "plot_valores_de_estado(V, ambiente)\n",
    "\n",
    "plot_valores_de_acao(Q)\n",
    "\n",
    "plot_policy(ambiente, politica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8f6d4",
   "metadata": {
    "id": "f6a8f6d4"
   },
   "source": [
    "# Tarefa:\n",
    "\n",
    "1. Modifique os códigos dos algoritmos de iteração de valor e de iteração de política para também retornar a iteração k em que a condição de convergência foi satisfeita.\n",
    "2. Compare os algoritmos de iteração de valor e de iteração de política quanto ao número de iterações utilizadas até a condição de convergência ser satisfeita.\n",
    "\n",
    "Entregar o PDF do notebook no colab (código + relatório em markdown)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
