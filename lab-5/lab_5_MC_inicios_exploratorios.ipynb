{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8441f52a",
   "metadata": {
    "id": "8441f52a"
   },
   "source": [
    "# Experimento 5:  MC com inícios exploratórios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298bf0e",
   "metadata": {
    "id": "4298bf0e"
   },
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4786f23b",
   "metadata": {
    "id": "4786f23b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from typing import Dict, Tuple, List, Union\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8610559",
   "metadata": {
    "id": "e8610559"
   },
   "source": [
    "## Ambiente: Navegação no Labirinto (gridworld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2766bd79",
   "metadata": {
    "id": "2766bd79"
   },
   "outputs": [],
   "source": [
    "class AmbienteNavegacaoLabirinto:\n",
    "    def __init__(self, world_size, bad_states, target_states, allow_bad_entry=False, rewards=[-1, -1, 1, 0]):\n",
    "        \"\"\"\n",
    "        Inicializa o ambiente de navegação em labirinto.\n",
    "\n",
    "        Parâmetros:\n",
    "        - world_size: tupla (n_linhas, n_colunas)\n",
    "        - bad_states: lista de tuplas com coordenadas de estados penalizados\n",
    "        - target_states: lista de tuplas com coordenadas dos estados de objetivo\n",
    "        - allow_bad_entry: bool, se False impede entrada em estados ruins (rebote)\n",
    "        - rewards: lista de recompensas com [r_boundary, r_bad, r_target, r_other]\n",
    "        \"\"\"\n",
    "        self.n_rows, self.n_cols = world_size   # dimensões da grade do labirinto\n",
    "        self.bad_states = set(bad_states)       # estados com penalidade alta\n",
    "        self.target_states = set(target_states) # estados com recompensa alta\n",
    "        self.allow_bad_entry = allow_bad_entry  # se o agente pode entrar em estados ruins\n",
    "\n",
    "        # Recompensas definidas para cada tipo de transição\n",
    "        self.r_boundary = rewards[0]  # tentar sair da grade\n",
    "        self.r_bad = rewards[1]       # transição para estado ruim\n",
    "        self.r_target = rewards[2]    # transição para estado alvo\n",
    "        self.r_other = rewards[3]     # demais transições\n",
    "\n",
    "        # Espaço de ações: dicionário com deslocamentos (linha, coluna)\n",
    "        self.action_space = {\n",
    "            0: (-1, 0),  # cima\n",
    "            1: (1, 0),   # baixo\n",
    "            2: (0, -1),  # esquerda\n",
    "            3: (0, 1),   # direita\n",
    "            4: (0, 0)    # permanecer no mesmo estado\n",
    "        }\n",
    "\n",
    "        # Espaço de recompensas: lista de recompensas possíveis\n",
    "        self.recompensas_possiveis = np.array(sorted(set(rewards)))\n",
    "        self.reward_map = {r: i for i, r in enumerate(self.recompensas_possiveis)}\n",
    "\n",
    "        # número total de estados\n",
    "        self.n_states = self.n_rows * self.n_cols\n",
    "\n",
    "        # número total de ações\n",
    "        self.n_actions = len(self.action_space)\n",
    "\n",
    "        # número total de recompensas possíveis\n",
    "        self.n_rewards = self.recompensas_possiveis.shape[0]\n",
    "\n",
    "        # Tensor de probabilidades de transição: P(s'|s,a)\n",
    "        self.state_transition_probabilities = np.zeros((self.n_states, self.n_states, self.n_actions))\n",
    "\n",
    "        # Tensor de probabilidade de recompensas: P(r|s,a)\n",
    "        self.reward_probabilities = np.zeros((self.n_rewards, self.n_states, self.n_actions))\n",
    "\n",
    "        # Matriz de recompensa imediata (determinística): recompensa[s, a] = r\n",
    "        self.recompensas_imediatas = np.zeros((self.n_states, self.n_actions))\n",
    "\n",
    "        # Matriz de transição de estados (determinística): transicao[s, a] = s'\n",
    "        self.transicao_de_estados = np.zeros((self.n_states, self.n_actions), dtype=int)\n",
    "\n",
    "        self.agent_pos = (0, 0)  # posição inicial do agente\n",
    "\n",
    "        self._init_dynamics()  # inicializa as dinâmicas de transição e recompensa\n",
    "\n",
    "\n",
    "    def _init_dynamics(self):\n",
    "        \"\"\"\n",
    "        Preenche as matrizes de transição e recompensa com base\n",
    "        na estrutura do ambiente e regras de movimentação.\n",
    "        \"\"\"\n",
    "        for indice_estado in range(self.n_states):\n",
    "            estado_atual = self.index_to_state(indice_estado)\n",
    "\n",
    "            for acao, (d_linha, d_coluna) in self.action_space.items():\n",
    "                proxima_posicao = (estado_atual[0] + d_linha, estado_atual[1] + d_coluna)\n",
    "\n",
    "                # Verifica se o movimento é válido ou resulta em rebote\n",
    "                if not self._in_bounds(proxima_posicao) or (not self.allow_bad_entry and proxima_posicao in self.bad_states):\n",
    "                    proximo_estado = estado_atual  # rebote: permanece no estado atual\n",
    "                else:\n",
    "                    proximo_estado = proxima_posicao\n",
    "\n",
    "                # Calcula a recompensa imediata da transição (s, a)\n",
    "                recompensa = self._compute_reward(proxima_posicao)\n",
    "\n",
    "                # Armazena a recompensa imediata na matriz\n",
    "                self.recompensas_imediatas[indice_estado, acao] = recompensa\n",
    "\n",
    "                # Ambiente determinístico\n",
    "                indice_proximo = self.state_to_index(proximo_estado)\n",
    "                self.state_transition_probabilities[indice_proximo, indice_estado, acao] = 1.0  # registra probabilidade P(s'|s,a)\n",
    "                indice_recompensa = self.reward_map[recompensa]\n",
    "                self.reward_probabilities[indice_recompensa, indice_estado, acao] = 1.0  # registra probabilidade P(r|s,a)\n",
    "\n",
    "                # Armazena transição determinística (s, a) -> s'\n",
    "                self.transicao_de_estados[indice_estado, acao] = indice_proximo\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia a posição do agente para o estado inicial (0, 0).\"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.agent_pos\n",
    "\n",
    "\n",
    "    def step(self, acao: int, *, linear: bool = False) -> Tuple[Union[int, Tuple[int, int]], float]:\n",
    "        \"\"\"\n",
    "        Executa uma ação no ambiente, atualiza a posição do agente e devolve o próximo estado no formato desejado.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        acao   : int\n",
    "            Índice da ação a ser executada (0‒4).\n",
    "        linear : bool, opcional (default = False)\n",
    "            False -> retorna o estado como tupla (linha, coluna).\n",
    "            True  -> retorna o estado como índice linear {0,...,self.n_states-1}.\n",
    "\n",
    "        Retorna\n",
    "        -------\n",
    "        proximo_estado : (linha, coluna) | int\n",
    "            Nova posição do agente (próximo estado) no formato especificado.\n",
    "        recompensa     : float\n",
    "            Recompensa imediata recebida.\n",
    "        \"\"\"\n",
    "        d_linha, d_coluna = self.action_space[acao]\n",
    "        linha_dest = self.agent_pos[0] + d_linha\n",
    "        coluna_dest = self.agent_pos[1] + d_coluna\n",
    "        destino = (linha_dest, coluna_dest)\n",
    "\n",
    "        # Rebote se fora dos limites ou entrada em bad_state proibida\n",
    "        if not self._in_bounds(destino) or (not self.allow_bad_entry and destino in self.bad_states):\n",
    "            destino = self.agent_pos  # permanece\n",
    "\n",
    "        recompensa = self._compute_reward(destino)\n",
    "        self.agent_pos = destino\n",
    "\n",
    "        # --- Formato de retorno ---\n",
    "        proximo_estado = self.state_to_index(destino) if linear else destino\n",
    "\n",
    "        return proximo_estado, recompensa\n",
    "\n",
    "\n",
    "    def _in_bounds(self, posicao):\n",
    "        \"\"\"Verifica se uma posição está dentro dos limites do labirinto.\"\"\"\n",
    "        linha, coluna = posicao\n",
    "        return 0 <= linha < self.n_rows and 0 <= coluna < self.n_cols\n",
    "\n",
    "\n",
    "    def _compute_reward(self, destino):\n",
    "        \"\"\"\n",
    "        Define a recompensa com base no destino proposto:\n",
    "        - r_boundary: fora do grid\n",
    "        - r_bad: célula ruim\n",
    "        - r_target: célula alvo\n",
    "        - r_other: demais casos\n",
    "        \"\"\"\n",
    "        if not self._in_bounds(destino):\n",
    "            return self.r_boundary\n",
    "        elif destino in self.bad_states:\n",
    "            return self.r_bad\n",
    "        elif destino in self.target_states:\n",
    "            return self.r_target\n",
    "        else:\n",
    "            return self.r_other\n",
    "\n",
    "\n",
    "    def reset_to_state(self, state, verify_bounds: bool = True):\n",
    "        \"\"\"\n",
    "        Teleporta o agente para 'state' sem reiniciar o episódio completo.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        state : (int, int) | int\n",
    "            - Tupla (linha, coluna)\n",
    "            ou\n",
    "            - Índice linear (int)\n",
    "        verify_bounds : bool\n",
    "            Se True, lança ValueError se o estado não for válido.\n",
    "\n",
    "        Retorna\n",
    "        -------\n",
    "        observation : object\n",
    "            A observação correspondente ao novo estado (a própria posição).\n",
    "        \"\"\"\n",
    "        # Converte índice -> tupla, se necessário\n",
    "        if isinstance(state, int):\n",
    "            state = self.index_to_state(state)\n",
    "\n",
    "        if verify_bounds and not self._in_bounds(state):\n",
    "            raise ValueError(f\"Estado {state} fora dos limites do labirinto\")\n",
    "\n",
    "        self.agent_pos = tuple(state)      # mantém tupla imutável\n",
    "\n",
    "        return self.agent_pos\n",
    "\n",
    "\n",
    "    def state_to_index(self, estado):\n",
    "        \"\"\"Converte coordenada (linha, coluna) para índice linear.\"\"\"\n",
    "        linha, coluna = estado\n",
    "        return linha * self.n_cols + coluna\n",
    "\n",
    "\n",
    "    def index_to_state(self, indice):\n",
    "        \"\"\"Converte índice linear para coordenada (linha, coluna).\"\"\"\n",
    "        return divmod(indice, self.n_cols)  # (linha, coluna) = (indice // self.n_cols, indice % self.n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100bbcf",
   "metadata": {
    "id": "e100bbcf"
   },
   "source": [
    "## Funções auxiliares para visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eedf6090",
   "metadata": {
    "id": "eedf6090"
   },
   "outputs": [],
   "source": [
    "def plot_policy(env, policy, ax=None, titulo=\"Política\"):\n",
    "    _, ax = _prepare_grid(env, ax=ax)\n",
    "\n",
    "    for (r, c), action in policy.items():\n",
    "        x, y = c + 0.5, r + 0.5\n",
    "        color = 'black'\n",
    "        lw = 1.5\n",
    "\n",
    "        if action == 0:\n",
    "            ax.arrow(x, y, dx=0, dy=-0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 1:\n",
    "            ax.arrow(x, y, dx=0, dy=0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 2:\n",
    "            ax.arrow(x, y, dx=-0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 3:\n",
    "            ax.arrow(x, y, dx=0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
    "        elif action == 4:\n",
    "            circ = patches.Circle((x, y), 0.1, edgecolor=color, facecolor='none', linewidth=lw)\n",
    "            ax.add_patch(circ)\n",
    "\n",
    "    ax.set_title(titulo)\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def _prepare_grid(env, ax=None, draw_cells=True):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(env.n_cols, env.n_rows))\n",
    "    ax.set_xlim(0, env.n_cols)\n",
    "    ax.set_ylim(0, env.n_rows)\n",
    "    ax.set_xticks(np.arange(0, env.n_cols + 1, 1))\n",
    "    ax.set_yticks(np.arange(0, env.n_rows + 1, 1))\n",
    "    ax.grid(True)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    if draw_cells:\n",
    "        for r in range(env.n_rows):\n",
    "            for c in range(env.n_cols):\n",
    "                cell = (r, c)\n",
    "                if cell in env.bad_states:\n",
    "                    color = 'red'\n",
    "                elif cell in env.target_states:\n",
    "                    color = 'green'\n",
    "                else:\n",
    "                    color = 'white'\n",
    "                rect = patches.Rectangle(xy=(c, r), width=1, height=1, facecolor=color, edgecolor='gray')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "    return (None, ax) if ax else (fig, ax)\n",
    "\n",
    "def plot_valores_de_estado(valores_estado, ambiente, ax=None, titulo=\"Valores de Estado (V(s))\", cbar=True):\n",
    "\n",
    "    dados = valores_estado.reshape(ambiente.n_rows, ambiente.n_cols)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(ambiente.n_cols, ambiente.n_rows))\n",
    "\n",
    "    sns.heatmap(\n",
    "        data=dados,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='bwr',\n",
    "        square=True,\n",
    "        cbar=cbar,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(titulo)\n",
    "    if ax is None:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_valores_de_acao(valores_de_acao):\n",
    "    Q_transposta = valores_de_acao.T\n",
    "    n_acoes, n_estados = Q_transposta.shape\n",
    "\n",
    "    plt.figure(figsize=(n_estados, n_acoes))\n",
    "    ax = sns.heatmap(\n",
    "        Q_transposta,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='bwr',\n",
    "        cbar=True,\n",
    "        square=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "    # Rótulos das colunas (estados)\n",
    "    ax.set_xticks(np.arange(n_estados) + 0.5)\n",
    "    ax.set_xticklabels([f\"s{i}\" for i in range(n_estados)], rotation=0)\n",
    "\n",
    "    # Rótulos das linhas (ações)\n",
    "    ax.set_yticks(np.arange(n_acoes) + 0.5)\n",
    "    ax.set_yticklabels([f\"a{i}\" for i in range(n_acoes)], rotation=0)\n",
    "\n",
    "    ax.set_xlabel(r\"Estados\")\n",
    "    ax.set_ylabel(r\"Ações\")\n",
    "    ax.set_title(r\"Valores de ação (Q(s, a) transposta)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_labirinto(ambiente):\n",
    "    \"\"\"\n",
    "    Visualiza o labirinto usando seaborn.heatmap sem ticks nos eixos.\n",
    "\n",
    "    Representa:\n",
    "    - Estado neutro: branco\n",
    "    - Estado ruim: vermelho\n",
    "    - Estado alvo: verde\n",
    "    \"\"\"\n",
    "    # Cria matriz com valores padrão (0 = neutro)\n",
    "    matriz = np.zeros((ambiente.n_rows, ambiente.n_cols), dtype=int)\n",
    "\n",
    "    # Marca os estados ruins como 1\n",
    "    for (r, c) in ambiente.bad_states:\n",
    "        matriz[r, c] = 1\n",
    "\n",
    "    # Marca os estados alvo como 2\n",
    "    for (r, c) in ambiente.target_states:\n",
    "        matriz[r, c] = 2\n",
    "\n",
    "    # Mapa de cores: branco = neutro, vermelho = ruim, verde = alvo\n",
    "    cmap = ListedColormap([\"white\", \"red\", \"green\"])\n",
    "\n",
    "    plt.figure(figsize=(ambiente.n_cols, ambiente.n_rows))\n",
    "    ax = sns.heatmap(\n",
    "        matriz,\n",
    "        cmap=cmap,\n",
    "        cbar=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "        square=True\n",
    "    )\n",
    "\n",
    "    # Remove todos os ticks e labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    ax.set_title(\"Visualização do Labirinto\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_visitas_log(n_visitas):\n",
    "    \"\"\"\n",
    "    Gera um gráfico de dispersão com escala logarítmica no eixo y\n",
    "    mostrando o número de visitas para cada par (s,a).\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    n_visitas : np.ndarray\n",
    "        Matriz de número de visitas de shape (n_states, n_actions).\n",
    "    \"\"\"\n",
    "    n_states, n_actions = n_visitas.shape\n",
    "    x = np.arange(n_states * n_actions)  # índice linear do par (s,a)\n",
    "    y = n_visitas.flatten()              # número de visitas\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(x, y, s=10, alpha=0.7)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Índice linear do par (s,a)\")\n",
    "    plt.ylabel(\"Número de visitas ao par (s,a)\")\n",
    "    plt.title(\"Frequência de visitas (escala log)\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wb6lLQfRWE6x",
   "metadata": {
    "id": "wb6lLQfRWE6x"
   },
   "source": [
    "## MC com inícios exploratórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fvTFwEaTQT4I",
   "metadata": {
    "id": "fvTFwEaTQT4I"
   },
   "outputs": [],
   "source": [
    "def gerar_episodio_es(\n",
    "    ambiente,\n",
    "    estado_inicial: int,\n",
    "    acao_inicial: int,\n",
    "    politica: Dict[int, int],\n",
    "    T: int,\n",
    ") -> List[Tuple[int, int, float]]:\n",
    "    \"\"\"\n",
    "    Gera um episódio de comprimento fixo T atendendo à condição\n",
    "    de inicios exploratórios — isto é, começando pelo par (estado_inicial, acao_inicial)\n",
    "    que é imposto — e em seguida seguindo a política atual.\n",
    "    Cada passo armazenado na trajetória contém a tupla (s_t, a_t, r_{t+1}).\n",
    "\n",
    "    ----------\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    ambiente : AmbienteNavegacaoLabirinto\n",
    "        Instância do gridworld.\n",
    "    estado_inicial : int\n",
    "        Índice linear do estado em que o episódio começa (s_0).\n",
    "    acao_inicial : int\n",
    "        Ação forçada no primeiro passo (a_0) para garantir cobertura de todos os pares (s,a).\n",
    "    politica : Dict[int, int]\n",
    "        Política determinística seguida do segundo passo em diante.\n",
    "    T : int\n",
    "        Horizonte fixo — número de passos do episódio (>= 1).\n",
    "\n",
    "    ----------\n",
    "    Retorno\n",
    "    -------\n",
    "    trajetoria\n",
    "    \"\"\"\n",
    "\n",
    "    ambiente.reset_to_state(estado_inicial)\n",
    "\n",
    "    # Código aqui\n",
    "\n",
    "    return trajetoria\n",
    "\n",
    "\n",
    "def mc_inicios_exploratorios(\n",
    "    ambiente,\n",
    "    gamma: float = 0.9,\n",
    "    n_episodios: int = 10_000,\n",
    "    horizonte_T: int = 50,\n",
    ") -> Tuple[np.ndarray,\n",
    "           Dict[Tuple[int,int],int],\n",
    "           np.ndarray]:\n",
    "    \"\"\"\n",
    "    Implementa o MC com Inícios Exploratórios para o AmbienteNavegacaoLabirinto.\n",
    "\n",
    "    Para cada episódio:\n",
    "      1. Sorteia uniformemente um par inicial (s_0, a_0) — condição de inícios exploratórios;\n",
    "      2. Gera um episódio de comprimento fixo (horizonte_T) seguindo a política corrente;\n",
    "      3. Percorre a trajetória de trás para frente, acumulando o retorno G e atualizando a estimativa de Q(s,a);\n",
    "      4. Imediatamente após cada atualização de Q(s_t, . ), executa a etapa de melhoria de política (determinística e gulosa).\n",
    "\n",
    "    A política é mantida internamente como dicionário {índice linear: ação} e convertida para {(linha, coluna): ação} apenas no retorno da função.\n",
    "\n",
    "    ----------\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    ambiente    : AmbienteNavegacaoLabirinto\n",
    "        Gridworld determinístico onde o agente navega.\n",
    "    gamma       : float, opcional (default=0.9)\n",
    "        Fator de desconto\n",
    "    n_episodios : int, opcional (default=10 000)\n",
    "        Número total de episódios a serem executados.\n",
    "    horizonte_T : int, opcional (default=50)\n",
    "        Comprimento fixo de cada episódio.\n",
    "\n",
    "    ----------\n",
    "    Retorno\n",
    "    ----------\n",
    "    Q                 : np.ndarray, shape = (n_states, n_actions)\n",
    "        Estimativas finais de Q(s,a) obtidas por média de retornos.\n",
    "    politica          : Dict[(int,int), int]\n",
    "        Política determinística ótima no formato {(linha, coluna): ação}.\n",
    "    numero_de_visitas : np.ndarray, shape = (n_states, n_actions)\n",
    "        Matriz com o número de visitas a cada par (s,a) durante todo o processo.\n",
    "    \"\"\"\n",
    "\n",
    "    n_estados, n_acoes = ambiente.n_states, ambiente.n_actions\n",
    "\n",
    "    # ---------- Inicializações ----------\n",
    "    Q                     = np.zeros((n_estados, n_acoes))\n",
    "    soma_dos_retornos     = np.zeros((n_estados, n_acoes))\n",
    "    numero_de_visitas     = np.zeros((n_estados, n_acoes))\n",
    "\n",
    "    politica: Dict[int, int] = {s: np.random.randint(n_acoes) for s in range(n_estados)}\n",
    "\n",
    "    # Código Aqui\n",
    "\n",
    "    return Q, politica, numero_de_visitas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ff6b9",
   "metadata": {
    "id": "ec7ff6b9"
   },
   "source": [
    "## Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d196b06c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 212,
     "status": "error",
     "timestamp": 1747864615202,
     "user": {
      "displayName": "Leonardo Brito",
      "userId": "11465166609065312365"
     },
     "user_tz": 180
    },
    "id": "d196b06c",
    "outputId": "c2fd0125-099c-47e8-c9ef-68b0ce5d5851"
   },
   "outputs": [],
   "source": [
    "# Instancia o ambiente\n",
    "ambiente = AmbienteNavegacaoLabirinto(\n",
    "        world_size=(5, 5),\n",
    "        bad_states=[(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)],\n",
    "        target_states=[(3, 2)],\n",
    "        allow_bad_entry=True,           # permite entrar em estados ruins\n",
    "        rewards=[-1, -10, 1, 0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e3daf",
   "metadata": {
    "id": "b80e3daf"
   },
   "source": [
    "## Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "QyTnKXQLWlyd",
   "metadata": {
    "id": "QyTnKXQLWlyd"
   },
   "outputs": [],
   "source": [
    "Q_es, politica_es, n_visitas = mc_inicios_exploratorios(\n",
    "    ambiente,             # gridworld\n",
    "    gamma=0.9,            # fator de desconto\n",
    "    n_episodios=10_000,   # número total de episódios\n",
    "    horizonte_T=100       # comprimento fixo de cada episódio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ntLVmxptrF-",
   "metadata": {
    "id": "6ntLVmxptrF-"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualização\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplot_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mambiente\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolitica_es\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitulo\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPolítica\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mplot_policy\u001b[39m\u001b[34m(env, policy, ax, titulo)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_policy\u001b[39m(env, policy, ax=\u001b[38;5;28;01mNone\u001b[39;00m, titulo=\u001b[33m\"\u001b[39m\u001b[33mPolítica\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      2\u001b[39m     _, ax = _prepare_grid(env, ax=ax)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (r, c), action \u001b[38;5;129;01min\u001b[39;00m policy.items():\n\u001b[32m      5\u001b[39m         x, y = c + \u001b[32m0.5\u001b[39m, r + \u001b[32m0.5\u001b[39m\n\u001b[32m      6\u001b[39m         color = \u001b[33m'\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable int object"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualização\n",
    "plot_policy(ambiente, politica_es, titulo=\"Política\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97K5PtT2xgQW",
   "metadata": {
    "id": "97K5PtT2xgQW"
   },
   "outputs": [],
   "source": [
    "plot_valores_de_acao(Q_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7M-C-RjBxgeB",
   "metadata": {
    "id": "7M-C-RjBxgeB"
   },
   "outputs": [],
   "source": [
    "plot_visitas_log(n_visitas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I6fmhL2hstkO",
   "metadata": {
    "id": "I6fmhL2hstkO"
   },
   "source": [
    "# Tarefa\n",
    "1. Implementar o método MC com inícios aleatórios.\n",
    "\n",
    "2. Analise o impacto o comprimento do episódio (T). Fixe n_episodios e o fator de desconto e varie T.\n",
    "\n",
    "3. Analise o impacto do número de episódios: Fixe T e o fator de desconto e varie n_episodios.\n",
    "\n",
    "4. Analise o impacto do número do fator de desconto: Fixe T e episodios e varie o fator de desconto.\n",
    "\n",
    "Após cada experimento, plote:\n",
    "\n",
    "    - número de visitas por (s,a)\n",
    "    \n",
    "    - política aprendida\n",
    "\n",
    "A tarefa deve ser entregue com:\n",
    "- código bem comentado (.ipynb),\n",
    "- gráficos comparativos,\n",
    "- breve relatório discutindo as observações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36zQy35h2g7s",
   "metadata": {
    "id": "36zQy35h2g7s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4298bf0e",
    "e8610559",
    "e100bbcf"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
