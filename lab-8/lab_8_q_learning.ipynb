{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ahVih93jpQ"
      },
      "source": [
        "# Biliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtRypzNFEb3t"
      },
      "outputs": [],
      "source": [
        "# Instala os pacotes necessários:\n",
        "# - gymnasium[toy-text]: inclui ambientes simples como FrozenLake, Taxi, etc.\n",
        "# - imageio[ffmpeg]: permite salvar vídeos e GIFs (formato .mp4 ou .gif)\n",
        "!pip install gymnasium[toy-text] imageio[ffmpeg]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTh_QK47EfwM"
      },
      "outputs": [],
      "source": [
        "# Importa as bibliotecas principais\n",
        "import gymnasium as gym               # Biblioteca de simulações de ambientes para RL\n",
        "import imageio                        # Usada para salvar a sequência de frames como GIF\n",
        "from IPython.display import Image     # Para exibir a imagem (GIF) diretamente no notebook\n",
        "import numpy as np                    # Importa o pacote NumPy, amplamente utilizado para manipulação de arrays e operações numéricas vetoriais\n",
        "from typing import Dict, Tuple, List  # Importa ferramentas de tipagem estática do Python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm            # barra de progresso\n",
        "print(gym.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação do Q-learning"
      ],
      "metadata": {
        "id": "lHca5D-vIXak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def executar_episodio_guloso(\n",
        "    env: gym.Env,                 # ambiente já instanciado\n",
        "    Q: np.ndarray,                # tabela Q (shape: |S| × |A|)\n",
        "    max_steps: int = 1_000        # limite de passos por segurança\n",
        ") -> Tuple[float, int]:\n",
        "    \"\"\"\n",
        "    Executa 1 episódio usando política determinística gulosa (ação = argmax_a Q(s,a)) e devolve a recompensa acumulada e o número de passos.\n",
        "    O laço termina se ocorrer qualquer destas condições:\n",
        "    1. `terminated`  – objetivo alcançado, segundo o ambiente.\n",
        "    2. `truncated`   – limite de passos imposto por `TimeLimit`.\n",
        "    3. Queda no penhasco (`reward == -100`), caso específico do Cliff Walking.\n",
        "    4. Número de passos ≥ `max_steps`.\n",
        "    \"\"\"\n",
        "    s, _ = env.reset()            # estado inicial\n",
        "    ret: float = 0.0              # recompensa acumulada\n",
        "    steps: int = 0                # contador de passos\n",
        "\n",
        "    # laço limitado a `max_steps`\n",
        "    for _ in range(max_steps):\n",
        "        a: int = int(np.argmax(Q[s]))          # ação gulosa\n",
        "        s, r, term, trunc, _ = env.step(a)     # transição ambiente\n",
        "        ret   += float(r)                      # acumula recompensa\n",
        "        steps += 1                             # incrementa passo\n",
        "\n",
        "        cliff: bool = (r == -100)              # caiu no precipício? (ambiente Cliff Walking)\n",
        "        if term or trunc or cliff:             # condição de parada\n",
        "            break\n",
        "\n",
        "    return ret, steps\n",
        "\n",
        "\n",
        "def q_learning(\n",
        "    env: gym.Env,                 # ambiente Gymnasium\n",
        "    episodes: int,                # número total de episódios\n",
        "    alpha: float,                 # taxa de aprendizado (passo do gradiente TD)\n",
        "    gamma: float,                 # fator de desconto\n",
        "    epsilon: float                # epsilon fixo da política comportamental\n",
        ") -> Tuple[np.ndarray,\n",
        "           List[int],\n",
        "           List[float],\n",
        "           Dict[int, np.ndarray]]:\n",
        "  '''\n",
        "    Retorna\n",
        "    -------\n",
        "    q_table          : np.ndarray           – matriz |S| × |A| com valores Q\n",
        "    episodio_len     : list[int]            – número de passos dos episódio\n",
        "    episodio_return  : list[float]          – retorno total dos episódio\n",
        "    target_policy    : dict[int, np.ndarray]– política determinística gulosa final  '''\n",
        "\n",
        "\n",
        "    # Implementacao aqui\n",
        "\n",
        "    return q_table, episodio_len, episodio_return, target_policy\n"
      ],
      "metadata": {
        "id": "p2NqK4_WqfN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaEmBiJL0i4u"
      },
      "source": [
        "# Visualização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbG3uhMd0vYn"
      },
      "outputs": [],
      "source": [
        "def plotar_metricas(\n",
        "    episodio_len: list[int],          # lista com o número de passos de cada episódio\n",
        "    episodio_return: list[float],     # lista com a recompensa total de cada episódio\n",
        "    janela: int = 100                 # tamanho da janela da média móvel\n",
        ") -> None:\n",
        "# Código aqui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzZ0D_e2vmP7"
      },
      "outputs": [],
      "source": [
        "ARROWS = {\n",
        "    \"FrozenLake-v1\":  ['←', '↓', '→', '↑'],  # 0=Left, 1=Down, 2=Right, 3=Up\n",
        "    \"CliffWalking-v0\": ['↑', '→', '↓', '←']  # 0=Up,   1=Right, 2=Down,  3=Left\n",
        "}\n",
        "\n",
        "def visualizar_politica(\n",
        "    env,\n",
        "    politica: Dict[int, np.ndarray],    # mapeamento estado → vetor de probabilidades (|A|)\n",
        "    shape: tuple[int, int]              # (linhas, colunas) da grade\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Desenha, com Matplotlib, histogramas das distribuições de probabilidade\n",
        "    de cada ação em cada estado de um ambiente toy-text do gymnasium.\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    env : gymnasium.Env\n",
        "        Ambiente já instanciado.\n",
        "    politica : Dict[int, np.ndarray]\n",
        "        Dicionário onde cada chave é um inteiro de estado e cada valor é um vetor 1-D com a distribuição epsilon-gulosa (ou gulosa) sobre as ações.\n",
        "    shape : tuple[int, int]\n",
        "        Formato da grade (número de linhas, número de colunas) para montagem dos subgráficos.\n",
        "    \"\"\"\n",
        "    # Descobre o id do ambiente original\n",
        "    id_env: str = env.unwrapped.spec.id\n",
        "\n",
        "    # Seleciona os rótulos de setas apropriados\n",
        "    try:\n",
        "        direcoes: list[str] = ARROWS[id_env]\n",
        "    except KeyError:                     # ambiente não mapeado\n",
        "        raise ValueError(f\"Ambiente “{id_env}” não suportado.\")\n",
        "\n",
        "    # Número de ações\n",
        "    n_acoes: int = len(direcoes)\n",
        "    acoes: np.ndarray = np.arange(n_acoes)  # [0, 1, 2, 3]\n",
        "\n",
        "    # Cria grid de subplots: uma subplot por estado\n",
        "    fig, axs = plt.subplots(shape[0], shape[1], figsize=(3 * shape[1], 2.2 * shape[0]))\n",
        "    axs = axs.flatten()  # facilita indexação linear\n",
        "\n",
        "    # Percorre cada estado existente na política\n",
        "    for s, ax in enumerate(axs):\n",
        "        pi: np.ndarray = politica[s]            # vetor de probabilidades\n",
        "\n",
        "        # Desenha barra com a probabilidade de cada ação\n",
        "        ax.bar(acoes, pi, color='gray')\n",
        "\n",
        "        # Configurações do eixo x\n",
        "        ax.set_xticks(acoes)\n",
        "        ax.set_xticklabels(direcoes)\n",
        "\n",
        "        # Limites e ticks do eixo y\n",
        "        ax.set_ylim(0, 1.05)\n",
        "        ax.set_yticks([0, 0.5, 1])\n",
        "\n",
        "        # Título identifica o número do estado\n",
        "        ax.set_title(f\"Estado {s}\")\n",
        "\n",
        "    # Ajusta layout para evitar sobreposição\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj1A2Isgr8Cn"
      },
      "outputs": [],
      "source": [
        "def gerar_gif(\n",
        "    politica: Dict[int, np.ndarray],             # estado → distribuição sobre ações\n",
        "    env: gym.Env,                                # ambiente com render_mode='rgb_array'\n",
        "    path_gif: str,                               # caminho/arquivo de saída (.gif)\n",
        "    n_episodios: int = 5                         # quantos episódios filmar\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Executa `n_episodios` usando a política fornecida (epsilon-gulosa ou gulosa) e gera um GIF mostrando a trajetória do agente.\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    politica : Dict[int, np.ndarray]\n",
        "        Para cada estado `s`, deve conter um vetor 1-D com as probabilidades de selecionar cada ação `a`.\n",
        "    env : gymnasium.Env\n",
        "        Ambiente já instanciado com `render_mode='rgb_array'`, pois `env.render()` retornará frames numéricos (arrays de pixels).\n",
        "    path_gif : str\n",
        "        Caminho completo do arquivo a ser salvo (ex.: `\"trajetoria.gif\"`).\n",
        "    n_episodios : int, opcional (default=5)\n",
        "        Número de episódios que serão simulados/gravados.\n",
        "\n",
        "    Retorno\n",
        "    -------\n",
        "    str\n",
        "        O mesmo `path_gif` recebido, para encadeamento ou conferência.\n",
        "    \"\"\"\n",
        "\n",
        "    frames: list[np.ndarray] = []                         # armazena todos os frames\n",
        "    n_actions: int = env.action_space.n\n",
        "\n",
        "\n",
        "    # --------------- coleta de frames ---------------\n",
        "    for _ in range(n_episodios):\n",
        "        state, _ = env.reset()                   # reinicia o ambiente\n",
        "        for _ in range(100):                     # limite de passos p/ não travar\n",
        "            # escolhe ação segundo a política\n",
        "            action = np.random.choice(n_actions, p=politica[state])\n",
        "\n",
        "            # executa ação e obtém próxima transição\n",
        "            state, _, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            # captura frame após passo do agente\n",
        "            frames.append(env.render())\n",
        "\n",
        "            # interrompe se objetivo alcançado (terminated) ou TimeLimit/erro (truncated)\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "    env.close()                                  # libera recursos do ambiente\n",
        "\n",
        "    # --------------- salva GIF ---------------\n",
        "    imageio.mimsave(path_gif, frames, format=\"GIF\", fps=2)\n",
        "\n",
        "    return path_gif                              # devolve caminho do arquivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmyeoebD12LK"
      },
      "source": [
        "# Simulação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwV8tBNfceSh"
      },
      "outputs": [],
      "source": [
        "# Configura o ambiente\n",
        "id=\"CliffWalking-v0\"\n",
        "render_mode = 'rgb_array'  # retorna imagens do ambiente como arrays de pixels\n",
        "env = gym.make(id=id, render_mode=render_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu06_B1pr73N"
      },
      "outputs": [],
      "source": [
        "# Hiper-parâmetros principais\n",
        "EPISODIOS = 1000     # @param {type:\"integer\"}\n",
        "ALPHA     = 0.1       # @param {type:\"number\"}\n",
        "GAMMA     = 0.9       # @param {type:\"number\"}\n",
        "EPSILON   = 0.1       # @param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-learning\n",
        "q_tab, ep_len, ep_ret, pi_eps = q_learning(\n",
        "    env,\n",
        "    episodes = EPISODIOS,\n",
        "    alpha    = ALPHA,\n",
        "    gamma    = GAMMA,\n",
        "    epsilon  = EPSILON\n",
        ")"
      ],
      "metadata": {
        "id": "fmH_PYEoJY7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMtXRVqOuYOM"
      },
      "outputs": [],
      "source": [
        "plotar_metricas(ep_len, ep_ret, janela=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI3ZOGKOvmSb"
      },
      "outputs": [],
      "source": [
        "visualizar_politica(env, pi_eps, shape=(4, 12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkeyHPZ-3KBx"
      },
      "outputs": [],
      "source": [
        "# Recria ambiente para renderizar\n",
        "id=\"CliffWalking-v0\"\n",
        "render_mode = 'rgb_array'  # retorna imagens do ambiente como arrays de pixels\n",
        "is_slippery = False  # torna o ambiente determinístico ou estocástico\n",
        "env = gym.make(id=id, render_mode=render_mode)\n",
        "\n",
        "# Gera o GIF\n",
        "gif_path = \"politica_epsilon_gulosa.gif\"\n",
        "gif_path = gerar_gif(pi_eps, env, path_gif=\"cliff_qlearning.gif\", n_episodios=5)\n",
        "\n",
        "# Exibe o GIF diretamente no notebook\n",
        "Image(filename=gif_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NraUVaHco8Hr"
      },
      "source": [
        "# Tarefa:\n",
        "\n",
        "1. Implemente o algoritmo Q-learning.\n",
        "2. Considere os 4 hiperparametros (EPISODIOS, ALPHA, GAMMA, EPSILON)\n",
        "- Varie um dos hiperparametros (ex.: EPISODIOS) e fixe os demais (ex.: ALPHA, GAMMA, EPSILON).\n",
        "- Plote a duração do episódio por episódio e também a recompensa total por episodio (ambas relativas à política alvo determinística gulosa - utilizar a função ``executar_episodio_guloso`` no final de cada episódio) para cada estudo de hiperparâmetro.\n",
        "- Observação: as curvas para cada estudo de hiperparâmetro devem estar na mesma figura, isto é, se o hiperparâmetro a ser variado é o EPSILON com 4 valores, entao o gráfico de duração do episódio deve mostrar as 4 curvas relativas a cada valor de EPSILON (com legenda) e de maneira similar para a recompensa total por episodio.\n",
        "3. Repita o procedimento para cada um dos hiperparametros.\n",
        "4. Reporte suas observações."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "f3ahVih93jpQ"
      ]
    },
    "kernelspec": {
      "display_name": "GAM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}