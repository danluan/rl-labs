{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KtRypzNFEb3t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gymnasium[toy-text]\n"
     ]
    }
   ],
   "source": [
    "# Instala os pacotes necessários:\n",
    "# - gymnasium[toy-text]: inclui ambientes simples como FrozenLake, Taxi, etc.\n",
    "# - imageio[ffmpeg]: permite salvar vídeos e GIFs (formato .mp4 ou .gif)\n",
    "!pip install gymnasium[toy-text] imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FTh_QK47EfwM"
   },
   "outputs": [],
   "source": [
    "# Importa as bibliotecas principais\n",
    "import gymnasium as gym               # Biblioteca de simulações de ambientes para RL\n",
    "import imageio                        # Usada para salvar a sequência de frames como GIF\n",
    "from IPython.display import Image     # Para exibir a imagem (GIF) diretamente no notebook\n",
    "import numpy as np                    # Importa o pacote NumPy, amplamente utilizado para manipulação de arrays e operações numéricas vetoriais\n",
    "from typing import Dict, Tuple, List  # Importa ferramentas de tipagem estática do Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kWokGGZYb617"
   },
   "outputs": [],
   "source": [
    "def avaliar_politica_truncada(\n",
    "    P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]],\n",
    "    politica: Dict[int, int],\n",
    "    n_estados: int,\n",
    "    gamma: float = 0.9,\n",
    "    j_truncado: int = 5,\n",
    "    V: np.ndarray | None = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Executa a avaliação truncada de uma política fixa (limitada a j_truncado iterações).\n",
    "\n",
    "    Esta versão não modifica o vetor V original (não é in-place) e assume V(s') = 0 para estados terminais s'.\n",
    "\n",
    "    Parâmetros:\n",
    "    - P: dicionário de transições do ambiente (env.P), com estrutura:\n",
    "         Dict[estado, Dict[ação, List[Tuple[probabilidade, próximo_estado, recompensa, terminal]]]]\n",
    "    - politica: dicionário que mapeia cada estado (int) para uma ação (int)\n",
    "    - n_estados: número total de estados\n",
    "    - gamma: fator de desconto (0 < gamma <= 1)\n",
    "    - j_truncado: número máximo de iterações de avaliação por rodada\n",
    "    - V: vetor de valores de estado (opcional). Se None, inicializa com zeros.\n",
    "\n",
    "    Retorna:\n",
    "    - V: vetor de valores de estado V (numpy array) para todos os estados\n",
    "    \"\"\"\n",
    "    if V is None:\n",
    "        V = np.zeros(n_estados)\n",
    "\n",
    "    ############################################################################################################\n",
    "    # AVALIAÇÃO DA POLÍTICA ATUAL\n",
    "    ############################################################################################################\n",
    "    # Código aqui\n",
    "\n",
    "    ############################################################################################################\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Htl0JkiNb6-Z"
   },
   "outputs": [],
   "source": [
    "def melhorar_politica(\n",
    "    P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]],\n",
    "    politica_atual: Dict[int, int],\n",
    "    V: np.ndarray,\n",
    "    n_estados: int,\n",
    "    n_acoes: int,\n",
    "    gamma: float = 0.9\n",
    ") -> Tuple[np.ndarray, Dict[int, int], bool]:\n",
    "    Q = np.zeros((n_estados, n_acoes))\n",
    "    nova_politica: Dict[int, int] = {}\n",
    "    politica_estavel = True\n",
    "\n",
    "    ############################################################################################################\n",
    "    # MELHORIA DA POLÍTICA\n",
    "    ############################################################################################################\n",
    "    # Código aqui\n",
    "\n",
    "    ############################################################################################################\n",
    "\n",
    "    return Q, nova_politica, politica_estavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CDZOT0ZLb7FQ"
   },
   "outputs": [],
   "source": [
    "def iteracao_de_politica_truncada(\n",
    "    env: gym.Env,\n",
    "    gamma: float = 0.9,\n",
    "    j_truncado: int = 5,\n",
    "    max_iteracoes: int = 1000\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[int, int]]:\n",
    "    \"\"\"\n",
    "    Executa o algoritmo de Iteração de Política com avaliação truncada para ambientes Gymnasium (baseados em env.P).\n",
    "\n",
    "    Parâmetros:\n",
    "    - env: ambiente compatível com Gymnasium que possui o atributo env.P\n",
    "    - gamma: fator de desconto (0 < gamma <= 1)\n",
    "    - j_truncado: número de iterações da avaliação de política por rodada (truncagem)\n",
    "    - max_iteracoes: número máximo de iterações de melhoria de política\n",
    "\n",
    "    Retorna:\n",
    "    - V: vetor de valores de estado V(s)\n",
    "    - Q: matriz de valores de ação Q(s,a)\n",
    "    - politica: dicionário estado → ação (int → int)\n",
    "    \"\"\"\n",
    "\n",
    "    env = env.unwrapped         # Garante acesso direto ao modelo\n",
    "\n",
    "    n_estados = env.observation_space.n\n",
    "    n_acoes = env.action_space.n\n",
    "    P = env.P\n",
    "\n",
    "    # Inicializa a política com ações aleatórias\n",
    "    politica: Dict[int, int] = {s: np.random.choice(n_acoes) for s in range(n_estados)}\n",
    "    V = np.zeros(n_estados)\n",
    "\n",
    "    for _ in range(max_iteracoes):\n",
    "        # Etapa 1: Avaliação truncada da política atual\n",
    "        V = avaliar_politica_truncada(P, politica, n_estados, gamma=gamma, j_truncado=j_truncado, V=V)\n",
    "\n",
    "        # Etapa 2: Melhoria da política\n",
    "        Q, nova_politica, politica_estavel = melhorar_politica(P, politica, V, n_estados, n_acoes, gamma=gamma)\n",
    "\n",
    "        # Se a política não mudou, convergimos\n",
    "        if politica_estavel:\n",
    "            break\n",
    "\n",
    "        # Atualiza política\n",
    "        politica = nova_politica\n",
    "\n",
    "    return V, Q, politica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c5VcAiF2e74_"
   },
   "outputs": [],
   "source": [
    "def visualizar_politica(politica: Dict[int, int], shape: Tuple[int, int]) -> None:\n",
    "    \"\"\"Exibe a política em uma grade com setas para cada ação.\"\"\"\n",
    "    direcoes = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "    grid = np.array([direcoes[politica[s]] if s in politica else ' ' for s in range(shape[0]*shape[1])])\n",
    "    print(\"\\nPolítica ótima:\")\n",
    "    print(grid.reshape(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CwV8tBNfceSh"
   },
   "outputs": [],
   "source": [
    "# Cria o ambiente FrozenLake\n",
    "# is_slippery=False: torna o ambiente determinístico (sem escorregões)\n",
    "# render_mode=\"rgb_array\": retorna imagens do ambiente como arrays de pixels\n",
    "# map_name='8x8':  tamanho do mapa (pode ser '4x4' ou '8x8')\n",
    "map_name = '4x4'\n",
    "render_mode=\"rgb_array\"\n",
    "is_slippery=False\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=map_name, render_mode=render_mode, is_slippery=is_slippery)\n",
    "env = env.unwrapped  # isso é ESSENCIAL para acessar env.P\n",
    "################################################################################\n",
    "# Estrutura de env.P\n",
    "################################################################################\n",
    "# env.P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]]\n",
    "# env.P = {\n",
    "#     estado_0: {\n",
    "#         acao_0: [(p, s', r, done), ...],\n",
    "#         acao_1: [(p, s', r, done), ...],\n",
    "#         ...\n",
    "#     },\n",
    "#     estado_1: {\n",
    "#         acao_0: [(p, s', r, done), ...],\n",
    "#         ...\n",
    "#     },\n",
    "#     ...\n",
    "# }\n",
    "# (p, s', r, done) = (probabilidade, proximo_estado, recompensa, finalizado)\n",
    "# probabilidade = p(s',r|s,a)\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vJLb-35ygg-2"
   },
   "outputs": [],
   "source": [
    "# Obter a política ótima\n",
    "_, _, politica_otima = iteracao_de_politica_truncada(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RRZNtmiyfDpY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Política ótima:\n",
      "[['↓' '←' '↓' '→']\n",
      " ['↑' '↑' '→' '↓']\n",
      " ['←' '↑' '→' '→']\n",
      " ['←' '←' '↓' '←']]\n"
     ]
    }
   ],
   "source": [
    "# Visualizar a poltíca ótima\n",
    "if map_name == '4x4':\n",
    "  shape = (4, 4)\n",
    "else:\n",
    "  shape=(8, 8)\n",
    "visualizar_politica(politica_otima, shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gJR2eCWZEuZe"
   },
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "pygame is not installed, run `pip install \"gymnasium[toy-text]\"`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ufrn/ref-learning/labs/.venv/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:348\u001b[39m, in \u001b[36mFrozenLakeEnv._render_gui\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pygame'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDependencyNotInstalled\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m action = politica_otima[observation]                                                              \u001b[38;5;66;03m# Seleciona a ação da política ótima\u001b[39;00m\n\u001b[32m      8\u001b[39m observation, reward, terminated, truncated, info = env.step(action)                               \u001b[38;5;66;03m# Aplica a ação no ambiente\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m frames.append(\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)                                                                       \u001b[38;5;66;03m# Captura a imagem do ambiente após a ação\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:                                                                       \u001b[38;5;66;03m# Verifica se o episódio acabou (chegou no objetivo ou caiu no buraco)\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ufrn/ref-learning/labs/.venv/lib/python3.12/site-packages/gymnasium/core.py:337\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ufrn/ref-learning/labs/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:409\u001b[39m, in \u001b[36mOrderEnforcing.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ufrn/ref-learning/labs/.venv/lib/python3.12/site-packages/gymnasium/core.py:337\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ufrn/ref-learning/labs/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:301\u001b[39m, in \u001b[36mPassiveEnvChecker.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checked_render \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_render = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.render()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ufrn/ref-learning/labs/.venv/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:361\u001b[39m, in \u001b[36menv_render_passive_checker\u001b[39m\u001b[34m(env)\u001b[39m\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    356\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m env.render_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env.render_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[32m    357\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    358\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv.render_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    359\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m result = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m env.render_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    363\u001b[39m     _check_render_return(env.render_mode, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ufrn/ref-learning/labs/.venv/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:344\u001b[39m, in \u001b[36mFrozenLakeEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._render_text()\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ufrn/ref-learning/labs/.venv/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:350\u001b[39m, in \u001b[36mFrozenLakeEnv._render_gui\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[32m    351\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpygame is not installed, run `pip install \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgymnasium[toy-text]\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    352\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.window_surface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    355\u001b[39m     pygame.init()\n",
      "\u001b[31mDependencyNotInstalled\u001b[39m: pygame is not installed, run `pip install \"gymnasium[toy-text]\"`"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=map_name, render_mode=render_mode, is_slippery=is_slippery)    # Cria o ambiente FrozenLake\n",
    "frames = []                                                                                             # Lista que armazenará todos os frames (imagens) do episódio\n",
    "n_episodios = 5                                                                                         # Número de episódios\n",
    "for ep in range(n_episodios):\n",
    "  observation, info = env.reset()                                                                       # Reinicia o ambiente e obtém o primeiro estado (observation)\n",
    "  for _ in range(100):                                                                                  # Executa um episódio de até 100 passos\n",
    "      action = politica_otima[observation]                                                              # Seleciona a ação da política ótima\n",
    "      observation, reward, terminated, truncated, info = env.step(action)                               # Aplica a ação no ambiente\n",
    "      frames.append(env.render())                                                                       # Captura a imagem do ambiente após a ação\n",
    "      if terminated or truncated:                                                                       # Verifica se o episódio acabou (chegou no objetivo ou caiu no buraco)\n",
    "          break\n",
    "env.close()                                                                                             # Encerra o ambiente corretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TO2p215TEuhP"
   },
   "outputs": [],
   "source": [
    "# Salva os frames coletados como um arquivo GIF animado\n",
    "gif_path = \"frozenlake.gif\"\n",
    "imageio.mimsave(gif_path, frames, format=\"GIF\", fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wxGGJ8zExzH"
   },
   "outputs": [],
   "source": [
    "# Exibe o GIF diretamente no notebook\n",
    "Image(filename=gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NraUVaHco8Hr"
   },
   "source": [
    "# Tarefa:\n",
    "\n",
    "1. Modifique o código do algoritmo de iteração de política truncada para também retornar a iteração k em que a condição de convergência foi satisfeita.\n",
    "2. Gere um gráfico de dispersão considerando (x,y) = (iteração em que a condição de convergência foi satisfeita, j_truncado).\n",
    "\n",
    "Utilize os mapas '4x4' e '8x8' nos experimentos e comente sobre os resultados obtidos nos itens 1 e 2.\n",
    "\n",
    "Entregar o PDF do notebook no colab (código + relatório em markdown)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
