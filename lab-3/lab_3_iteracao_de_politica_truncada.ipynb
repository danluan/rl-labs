{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KtRypzNFEb3t"},"outputs":[],"source":["# Instala os pacotes necessários:\n","# - gymnasium[toy-text]: inclui ambientes simples como FrozenLake, Taxi, etc.\n","# - imageio[ffmpeg]: permite salvar vídeos e GIFs (formato .mp4 ou .gif)\n","!pip install gymnasium[toy-text] imageio[ffmpeg]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTh_QK47EfwM"},"outputs":[],"source":["# Importa as bibliotecas principais\n","import gymnasium as gym               # Biblioteca de simulações de ambientes para RL\n","import imageio                        # Usada para salvar a sequência de frames como GIF\n","from IPython.display import Image     # Para exibir a imagem (GIF) diretamente no notebook\n","import numpy as np                    # Importa o pacote NumPy, amplamente utilizado para manipulação de arrays e operações numéricas vetoriais\n","from typing import Dict, Tuple, List  # Importa ferramentas de tipagem estática do Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWokGGZYb617"},"outputs":[],"source":["def avaliar_politica_truncada(\n","    P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]],\n","    politica: Dict[int, int],\n","    n_estados: int,\n","    gamma: float = 0.9,\n","    j_truncado: int = 5,\n","    V: np.ndarray | None = None\n",") -> np.ndarray:\n","    \"\"\"\n","    Executa a avaliação truncada de uma política fixa (limitada a j_truncado iterações).\n","\n","    Esta versão não modifica o vetor V original (não é in-place) e assume V(s') = 0 para estados terminais s'.\n","\n","    Parâmetros:\n","    - P: dicionário de transições do ambiente (env.P), com estrutura:\n","         Dict[estado, Dict[ação, List[Tuple[probabilidade, próximo_estado, recompensa, terminal]]]]\n","    - politica: dicionário que mapeia cada estado (int) para uma ação (int)\n","    - n_estados: número total de estados\n","    - gamma: fator de desconto (0 < gamma <= 1)\n","    - j_truncado: número máximo de iterações de avaliação por rodada\n","    - V: vetor de valores de estado (opcional). Se None, inicializa com zeros.\n","\n","    Retorna:\n","    - V: vetor de valores de estado V (numpy array) para todos os estados\n","    \"\"\"\n","    if V is None:\n","        V = np.zeros(n_estados)\n","\n","    ############################################################################################################\n","    # AVALIAÇÃO DA POLÍTICA ATUAL\n","    ############################################################################################################\n","    # Código aqui\n","\n","    ############################################################################################################\n","\n","    return V"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Htl0JkiNb6-Z"},"outputs":[],"source":["def melhorar_politica(\n","    P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]],\n","    politica_atual: Dict[int, int],\n","    V: np.ndarray,\n","    n_estados: int,\n","    n_acoes: int,\n","    gamma: float = 0.9\n",") -> Tuple[np.ndarray, Dict[int, int], bool]:\n","    Q = np.zeros((n_estados, n_acoes))\n","    nova_politica: Dict[int, int] = {}\n","    politica_estavel = True\n","\n","    ############################################################################################################\n","    # MELHORIA DA POLÍTICA\n","    ############################################################################################################\n","    # Código aqui\n","\n","    ############################################################################################################\n","\n","    return Q, nova_politica, politica_estavel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDZOT0ZLb7FQ"},"outputs":[],"source":["def iteracao_de_politica_truncada(\n","    env: gym.Env,\n","    gamma: float = 0.9,\n","    j_truncado: int = 5,\n","    max_iteracoes: int = 1000\n",") -> Tuple[np.ndarray, np.ndarray, Dict[int, int]]:\n","    \"\"\"\n","    Executa o algoritmo de Iteração de Política com avaliação truncada para ambientes Gymnasium (baseados em env.P).\n","\n","    Parâmetros:\n","    - env: ambiente compatível com Gymnasium que possui o atributo env.P\n","    - gamma: fator de desconto (0 < gamma <= 1)\n","    - j_truncado: número de iterações da avaliação de política por rodada (truncagem)\n","    - max_iteracoes: número máximo de iterações de melhoria de política\n","\n","    Retorna:\n","    - V: vetor de valores de estado V(s)\n","    - Q: matriz de valores de ação Q(s,a)\n","    - politica: dicionário estado → ação (int → int)\n","    \"\"\"\n","\n","    env = env.unwrapped         # Garante acesso direto ao modelo\n","\n","    n_estados = env.observation_space.n\n","    n_acoes = env.action_space.n\n","    P = env.P\n","\n","    # Inicializa a política com ações aleatórias\n","    politica: Dict[int, int] = {s: np.random.choice(n_acoes) for s in range(n_estados)}\n","    V = np.zeros(n_estados)\n","\n","    for _ in range(max_iteracoes):\n","        # Etapa 1: Avaliação truncada da política atual\n","        V = avaliar_politica_truncada(P, politica, n_estados, gamma=gamma, j_truncado=j_truncado, V=V)\n","\n","        # Etapa 2: Melhoria da política\n","        Q, nova_politica, politica_estavel = melhorar_politica(P, politica, V, n_estados, n_acoes, gamma=gamma)\n","\n","        # Se a política não mudou, convergimos\n","        if politica_estavel:\n","            break\n","\n","        # Atualiza política\n","        politica = nova_politica\n","\n","    return V, Q, politica"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5VcAiF2e74_"},"outputs":[],"source":["def visualizar_politica(politica: Dict[int, int], shape: Tuple[int, int]) -> None:\n","    \"\"\"Exibe a política em uma grade com setas para cada ação.\"\"\"\n","    direcoes = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n","    grid = np.array([direcoes[politica[s]] if s in politica else ' ' for s in range(shape[0]*shape[1])])\n","    print(\"\\nPolítica ótima:\")\n","    print(grid.reshape(shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwV8tBNfceSh"},"outputs":[],"source":["# Cria o ambiente FrozenLake\n","# is_slippery=False: torna o ambiente determinístico (sem escorregões)\n","# render_mode=\"rgb_array\": retorna imagens do ambiente como arrays de pixels\n","# map_name='8x8':  tamanho do mapa (pode ser '4x4' ou '8x8')\n","map_name = '4x4'\n","render_mode=\"rgb_array\"\n","is_slippery=False\n","env = gym.make(\"FrozenLake-v1\", map_name=map_name, render_mode=render_mode, is_slippery=is_slippery)\n","env = env.unwrapped  # isso é ESSENCIAL para acessar env.P\n","################################################################################\n","# Estrutura de env.P\n","################################################################################\n","# env.P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]]\n","# env.P = {\n","#     estado_0: {\n","#         acao_0: [(p, s', r, done), ...],\n","#         acao_1: [(p, s', r, done), ...],\n","#         ...\n","#     },\n","#     estado_1: {\n","#         acao_0: [(p, s', r, done), ...],\n","#         ...\n","#     },\n","#     ...\n","# }\n","# (p, s', r, done) = (probabilidade, proximo_estado, recompensa, finalizado)\n","# probabilidade = p(s',r|s,a)\n","################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJLb-35ygg-2"},"outputs":[],"source":["# Obter a política ótima\n","_, _, politica_otima = iteracao_de_politica_truncada(env)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRZNtmiyfDpY"},"outputs":[],"source":["# Visualizar a poltíca ótima\n","if map_name == '4x4':\n","  shape = (4, 4)\n","else:\n","  shape=(8, 8)\n","visualizar_politica(politica_otima, shape=shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJR2eCWZEuZe"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", map_name=map_name, render_mode=render_mode, is_slippery=is_slippery)    # Cria o ambiente FrozenLake\n","frames = []                                                                                             # Lista que armazenará todos os frames (imagens) do episódio\n","n_episodios = 5                                                                                         # Número de episódios\n","for ep in range(n_episodios):\n","  observation, info = env.reset()                                                                       # Reinicia o ambiente e obtém o primeiro estado (observation)\n","  for _ in range(100):                                                                                  # Executa um episódio de até 100 passos\n","      action = politica_otima[observation]                                                              # Seleciona a ação da política ótima\n","      observation, reward, terminated, truncated, info = env.step(action)                               # Aplica a ação no ambiente\n","      frames.append(env.render())                                                                       # Captura a imagem do ambiente após a ação\n","      if terminated or truncated:                                                                       # Verifica se o episódio acabou (chegou no objetivo ou caiu no buraco)\n","          break\n","env.close()                                                                                             # Encerra o ambiente corretamente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TO2p215TEuhP"},"outputs":[],"source":["# Salva os frames coletados como um arquivo GIF animado\n","gif_path = \"frozenlake.gif\"\n","imageio.mimsave(gif_path, frames, format=\"GIF\", fps=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wxGGJ8zExzH"},"outputs":[],"source":["# Exibe o GIF diretamente no notebook\n","Image(filename=gif_path)"]},{"cell_type":"markdown","metadata":{"id":"NraUVaHco8Hr"},"source":["# Tarefa:\n","\n","1. Modifique o código do algoritmo de iteração de política truncada para também retornar a iteração k em que a condição de convergência foi satisfeita.\n","2. Gere um gráfico de dispersão considerando (x,y) = (iteração em que a condição de convergência foi satisfeita, j_truncado).\n","\n","Utilize os mapas '4x4' e '8x8' nos experimentos e comente sobre os resultados obtidos nos itens 1 e 2.\n","\n","Entregar o PDF do notebook no colab (código + relatório em markdown)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"GAM","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.15"}},"nbformat":4,"nbformat_minor":0}